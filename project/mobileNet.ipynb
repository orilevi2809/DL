{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv3/7zyJzov/G7lpTfUeP2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orilevi2809/DL/blob/main/project/mobileNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h9XCUtKcVha"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from torchvision import models, transforms\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224 #TODO: check if we can minize the size of the image e.g. 32*32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "img_dir = \"./labels/\"\n",
        "labels_dict = {\"crimp\": 1, \"pinch\": 2, \"pocket\": 4, \"jug\": 8, \"edge\": 16, \"sloper\": 32}\n",
        "\n",
        "# Define the transform for image preprocessing\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.3),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "wKg32gWBdKfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, model_file):\n",
        "    # Save the model's state dictionary to disk\n",
        "    torch.save(model.state_dict(), model_file)\n"
      ],
      "metadata": {
        "id": "AKU1xHdSdWNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_paths(img_dir, labels):\n",
        "    data = []\n",
        "    target = []\n",
        "    label_count = {}  # Dictionary to store the count of images for each label\n",
        "    for label in labels:\n",
        "        label_dir = os.path.join(img_dir, label)\n",
        "        count = 0  # Initialize the count for the current label\n",
        "        for filename in os.listdir(label_dir):\n",
        "            if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\")or filename.endswith(\".png\"):\n",
        "                img_path = os.path.join(label_dir, filename)\n",
        "                data.append(img_path)\n",
        "                target.append(labels_dict[label])\n",
        "                count += 1  # Increment the count for the current label\n",
        "        label_count[label] = count  # Store the count in the dictionary\n",
        "\n",
        "    # Print the count of images for each label\n",
        "    for label, count in label_count.items():\n",
        "        print(f\"Number of images for label '{label}': {count}\")\n",
        "    return data, target\n"
      ],
      "metadata": {
        "id": "3m7EQoTqdZQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_data(img_paths, labels, transform, augment_unbalanced=False):\n",
        "    if augment_unbalanced:\n",
        "        data = []\n",
        "        target = []\n",
        "        label_counts = {label: 0 for label in set(labels)}\n",
        "        for img_path, label in zip(img_paths, labels):\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            img = test_transform(img)\n",
        "            data.append(img)\n",
        "            target.append(label)\n",
        "            label_counts[label] += 1\n",
        "\n",
        "        is_unbalanced = any(count < max(label_counts.values()) for count in label_counts.values())\n",
        "        print(label_counts)\n",
        "\n",
        "        if is_unbalanced:\n",
        "            print(\"unbalanced\")\n",
        "            augmented_data = []\n",
        "            augmented_target = []\n",
        "            #TODO: add while all equal\n",
        "            for img_path, label in zip(img_paths, labels):\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                augmented_data.append(test_transform(img))\n",
        "                augmented_target.append(label)\n",
        "\n",
        "                if label_counts[label] < max(label_counts.values()):\n",
        "                    augmented_img = train_transform(img)\n",
        "                    augmented_data.append(augmented_img)\n",
        "                    augmented_target.append(label)\n",
        "                    label_counts[label] += 1\n",
        "            print(label_counts)\n",
        "            return augmented_data, augmented_target\n",
        "        return data, target\n",
        "    else:\n",
        "        if isinstance(img_paths[0], torch.Tensor):  # Check if data is already in tensor format\n",
        "            return img_paths, labels\n",
        "        else:\n",
        "            data = []\n",
        "            target = []\n",
        "            for img_path, label in zip(img_paths, labels):\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                img = test_transform(img)\n",
        "                data.append(img)\n",
        "                target.append(label)\n",
        "            return data, target"
      ],
      "metadata": {
        "id": "wJ_D0fcgddJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_classifier(train_dataloader, val_dataloader, model, criterion, optimizer, device):\n",
        "    for epoch in range(10):  # 10 is the best!!\n",
        "        running_loss = 0.0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # Switch model to training mode\n",
        "        model.train()\n",
        "\n",
        "        for i, data in enumerate(train_dataloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate the number of correct predictions in the current batch\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "        train_accuracy = 100 * total_correct / total_samples\n",
        "\n",
        "        # Switch model to evaluation mode for validation\n",
        "        model.eval()\n",
        "\n",
        "        val_correct = 0\n",
        "        val_samples = 0\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_dataloader, 0):\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                val_samples += labels.size(0)\n",
        "\n",
        "        val_accuracy = 100 * val_correct / val_samples\n",
        "\n",
        "        print(f'Epoch: {epoch + 1}, Training loss: {running_loss / len(train_dataloader)}, Training accuracy: {train_accuracy}%, Validation loss: {val_loss / len(val_dataloader)}, Validation accuracy: {val_accuracy}%')\n",
        "\n",
        "    print('Finished Training')\n",
        "    return model"
      ],
      "metadata": {
        "id": "njyRGLd0dl_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classifier(test_dataloader, model, device):\n",
        "    model.eval()  # Switch model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in test_dataloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.numpy())\n",
        "            all_predictions.extend(predicted.numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the test images: {accuracy}%')\n",
        "    # Generate classification report and confusion matrix\n",
        "    report = classification_report(all_labels, all_predictions, zero_division=1)\n",
        "    matrix = confusion_matrix(all_labels, all_predictions)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(matrix)\n"
      ],
      "metadata": {
        "id": "9_N6kS2edofh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "####### pre prosses data #TODO:refactor to preprocces\n",
        "img_paths, labels = load_images_paths(img_dir, labels_dict.keys())\n",
        "\n",
        "# Split the data into train, validation, and test sets (e.g., 70%, 15%, 15%)\n",
        "img_paths_train, img_paths_test, labels_train, labels_test = train_test_split(img_paths, labels, test_size=0.3,stratify=labels, random_state=42)\n",
        "img_paths_val, img_paths_test, labels_val, labels_test = train_test_split(img_paths_test, labels_test,test_size=0.5, stratify=labels_test,random_state=42)\n",
        "\n",
        "data_train, labels_train = load_image_data(img_paths_train, labels_train, train_transform,augment_unbalanced=True)\n",
        "data_val, labels_val = load_image_data(img_paths_val, labels_val, test_transform,augment_unbalanced=False)\n",
        "data_test, labels_test = load_image_data(img_paths_test, labels_test, test_transform,augment_unbalanced=False)\n",
        "\n",
        "# Convert to tensors\n",
        "data_train = torch.stack(data_train)\n",
        "labels_train = torch.tensor(labels_train)\n",
        "\n",
        "data_val = torch.stack(data_val)\n",
        "labels_val = torch.tensor(labels_val)\n",
        "\n",
        "data_test = torch.stack(data_test)\n",
        "labels_test = torch.tensor(labels_test)\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(data_train, labels_train)\n",
        "val_dataset = TensorDataset(data_val, labels_val)\n",
        "test_dataset = TensorDataset(data_test, labels_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "-4Qhoj_OdsV_",
        "outputId": "2745deb7-d194-480c-a1ca-7e9f4647fe35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-e915d9c19a6e>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    data_val = torch.stack(data_val)A\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained MobileNetV2 model and replace the classifier layer\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "#for param in model.parameters():\n",
        "#    param.requires_grad = False\n",
        "\n",
        "# Enable gradient computation for all parameters in the classifier layers\n",
        "#for param in model.classifier.parameters():\n",
        "#    param.requires_grad = True\n",
        "\n",
        "# Only train the last three layers\n",
        "#for param in model.classifier[-3:].parameters():\n",
        "#    param.requires_grad = True\n",
        "\n",
        "model.classifier[1] = nn.Linear(model.last_channel, len(labels_dict))\n",
        "model = model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "TjJUKY51eBJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss criterion and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "\n",
        "model = train_classifier(train_dataloader, val_dataloader, model, criterion, optimizer, device)\n"
      ],
      "metadata": {
        "id": "3RmBQRq7eHib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if not os.path.exists('hold_classifier_MobileNet.pth'):\n",
        "    # Save the MobileNetV2 model\n",
        "    model_file = 'hold_classifier_MobileNet.pth'\n",
        "    save_model(model, model_file)\n",
        "\n",
        "    # Load the model for inference\n",
        "    model.load_state_dict(torch.load('hold_classifier_MobileNet.pth'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S7Uot81eeKEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_classifier(test_dataloader, model, device)"
      ],
      "metadata": {
        "id": "JsWOs-vqeMSM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}