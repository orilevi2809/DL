{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "mount_file_id": "1e0ncpuTeKvEQlAARGbsosFbB4BrLwDYp",
      "authorship_tag": "ABX9TyPEod4D5pOol2IJ3n8nDyMr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orilevi2809/DL/blob/main/project/mobileNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-h9XCUtKcVha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb7761a8-f223-4583-e27e-31e09794e59b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "!pip install timm\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224 #TODO: check if we can minize the size of the image e.g. 32*32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "img_dir = \"./drive/MyDrive/labels/\"\n",
        "labels_dict = {\"crimp\": 0, \"pinch\": 1, \"pocket\": 2, \"jug\": 3, \"edge\": 4, \"sloper\": 5}\n",
        "\n",
        "# Define the transform for image preprocessing\n",
        "no_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "no_transform_norm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.3),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_transform_norm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.3),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_transform2_norm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.1),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "wKg32gWBdKfR"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, model_file):\n",
        "    # Save the model's state dictionary to disk\n",
        "    torch.save(model.state_dict(), model_file)\n"
      ],
      "metadata": {
        "id": "AKU1xHdSdWNc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_paths(img_dir, labels):\n",
        "    data = []\n",
        "    target = []\n",
        "    label_count = {}  # Dictionary to store the count of images for each label\n",
        "    for label in labels:\n",
        "        label_dir = os.path.join(img_dir, label)\n",
        "        count = 0  # Initialize the count for the current label\n",
        "        for filename in os.listdir(label_dir):\n",
        "            if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\")or filename.endswith(\".png\"):\n",
        "                img_path = os.path.join(label_dir, filename)\n",
        "                data.append(img_path)\n",
        "                target.append(labels_dict[label])\n",
        "                count += 1  # Increment the count for the current label\n",
        "        label_count[label] = count  # Store the count in the dictionary\n",
        "\n",
        "    # Print the count of images for each label\n",
        "    for label, count in label_count.items():\n",
        "        print(f\"Number of images for label '{label}': {count}\")\n",
        "    return data, target , label_count\n"
      ],
      "metadata": {
        "id": "3m7EQoTqdZQy"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img):\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = img.numpy()\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "        img = (img * 255).astype(np.uint8)\n",
        "        plt.imshow(img)\n",
        "    else:  # Assuming img is a PIL.Image object\n",
        "        plt.imshow(img)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "d2ASLAkPFOSe"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_data(img_paths, labels, transform,label_counts, augment_unbalanced=False):\n",
        "    if augment_unbalanced:\n",
        "        is_unbalanced = any(count < max(label_counts.values()) for count in label_counts.values())\n",
        "        print(label_counts)\n",
        "        inv_labels_dict = {0:\"crimp\",1: \"pinch\",2: \"pocket\",3: \"jug\",4: \"edge\",5: \"sloper\"}\n",
        "\n",
        "        if is_unbalanced:\n",
        "            print(\"unbalanced\")\n",
        "            augmented_data = []\n",
        "            augmented_target = []\n",
        "            #TODO: add while all equal\n",
        "            for img_path, label in zip(img_paths, labels):\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                augmented_data.append(no_transform_norm(img))\n",
        "                augmented_target.append(label)\n",
        "                if label_counts[label] < max(label_counts.values()):\n",
        "                    augmented_img = train_transform_norm(img)\n",
        "                    #imshow(augmented_img)\n",
        "                    augmented_data.append(augmented_img)\n",
        "                    augmented_target.append(label)\n",
        "                    label_counts[label] += 1\n",
        "                if label_counts[label] < max(label_counts.values()):\n",
        "                    augmented_img = train_transform2_norm(img)\n",
        "                    #imshow(img)\n",
        "                    #imshow(augmented_img)\n",
        "                    augmented_data.append(augmented_img)\n",
        "                    augmented_target.append(label)\n",
        "                    label_counts[label] += 1\n",
        "            print(label_counts)\n",
        "            return augmented_data, augmented_target\n",
        "        else:\n",
        "              data = []\n",
        "              target = []\n",
        "              label_counts = {label: 0 for label in set(labels)}\n",
        "              for img_path, label in zip(img_paths, labels):\n",
        "                  img = Image.open(img_path).convert(\"RGB\")\n",
        "                  img = no_transform_norm(img)\n",
        "                  data.append(img)\n",
        "                  target.append(label)\n",
        "                  label_counts[label] += 1\n",
        "              return data, target\n",
        "    else:\n",
        "        if isinstance(img_paths[0], torch.Tensor):  # Check if data is already in tensor format\n",
        "            return img_paths, labels\n",
        "        else:\n",
        "            data = []\n",
        "            target = []\n",
        "            for img_path, label in zip(img_paths, labels):\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                img = no_transform_norm(img)\n",
        "                data.append(img)\n",
        "                target.append(label)\n",
        "            return data, target"
      ],
      "metadata": {
        "id": "wJ_D0fcgddJ8"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_classifier(train_dataloader, val_dataloader, model, criterion, optimizer, device,epochs):\n",
        "    for epoch in range(epochs):  # 10 is the best!!\n",
        "        running_loss = 0.0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # Switch model to training mode\n",
        "        model.train()\n",
        "\n",
        "        for i, data in enumerate(train_dataloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate the number of correct predictions in the current batch\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "        train_accuracy = 100 * total_correct / total_samples\n",
        "\n",
        "        # Switch model to evaluation mode for validation\n",
        "        model.eval()\n",
        "\n",
        "        val_correct = 0\n",
        "        val_samples = 0\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_dataloader, 0):\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                val_samples += labels.size(0)\n",
        "\n",
        "        val_accuracy = 100 * val_correct / val_samples\n",
        "\n",
        "        print(f'Epoch: {epoch + 1}, Training loss: {running_loss / len(train_dataloader)}, Training accuracy: {train_accuracy}%, Validation loss: {val_loss / len(val_dataloader)}, Validation accuracy: {val_accuracy}%')\n",
        "\n",
        "    print('Finished Training')\n",
        "    return model"
      ],
      "metadata": {
        "id": "njyRGLd0dl_I"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classifier(test_dataloader, model, device):\n",
        "    model.eval()  # Switch model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in test_dataloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            # Move tensors to CPU before converting to numpy arrays\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the test images: {accuracy}%')\n",
        "    # Generate classification report and confusion matrix\n",
        "    report = classification_report(all_labels, all_predictions, zero_division=1)\n",
        "    matrix = confusion_matrix(all_labels, all_predictions)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(matrix)\n"
      ],
      "metadata": {
        "id": "9_N6kS2edofh"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_paths, labels,_ = load_images_paths(img_dir, labels_dict.keys())\n",
        "print(\"total data= \", len(img_paths))\n",
        "\n",
        "# Split the data into train, validation, and test sets (e.g., 70%, 15%, 15%)\n",
        "img_paths_train, img_paths_test, labels_train, labels_test = train_test_split(img_paths, labels, test_size=0.3,stratify=labels, random_state=42)\n",
        "img_paths_val, img_paths_test, labels_val, labels_test = train_test_split(img_paths_test, labels_test,test_size=0.5, stratify=labels_test,random_state=42)\n",
        "\n",
        "\n",
        "label_counts = {label: 0 for label in set(labels)}\n",
        "for label in labels_train:\n",
        "  label_counts[label] += 1\n",
        "\n",
        "data_train, labels_train = load_image_data(img_paths_train, labels_train, train_transform,label_counts,augment_unbalanced=True)\n",
        "data_val, labels_val = load_image_data(img_paths_val, labels_val, no_transform,label_counts,augment_unbalanced=False)\n",
        "data_test, labels_test = load_image_data(img_paths_test, labels_test, no_transform,label_counts,augment_unbalanced=False)\n",
        "\n",
        "\n",
        "# Convert to tensors\n",
        "data_train = torch.stack(data_train)\n",
        "labels_train = torch.tensor(labels_train)\n",
        "\n",
        "data_val = torch.stack(data_val)\n",
        "labels_val = torch.tensor(labels_val)\n",
        "\n",
        "data_test = torch.stack(data_test)\n",
        "labels_test = torch.tensor(labels_test)\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(data_train, labels_train)\n",
        "val_dataset = TensorDataset(data_val, labels_val)\n",
        "test_dataset = TensorDataset(data_test, labels_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "-4Qhoj_OdsV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b622e30-5e5c-4415-e6d7-8b2156cd329c"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images for label 'crimp': 161\n",
            "Number of images for label 'pinch': 485\n",
            "Number of images for label 'pocket': 178\n",
            "Number of images for label 'jug': 101\n",
            "Number of images for label 'edge': 113\n",
            "Number of images for label 'sloper': 262\n",
            "total data=  1300\n",
            "{0: 113, 1: 339, 2: 125, 3: 71, 4: 79, 5: 183}\n",
            "unbalanced\n",
            "{0: 339, 1: 339, 2: 339, 3: 213, 4: 237, 5: 339}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the data loaders\n",
        "pickle_dir = \"./drive/MyDrive/pickles/\"\n",
        "\n",
        "with open(pickle_dir+'train_dataloader.pkl', 'wb') as f:\n",
        "    pickle.dump(train_dataloader, f)\n",
        "\n",
        "with open(pickle_dir+'val_dataloader.pkl', 'wb') as f:\n",
        "    pickle.dump(val_dataloader, f)\n",
        "\n",
        "with open(pickle_dir+'test_dataloader.pkl', 'wb') as f:\n",
        "    pickle.dump(test_dataloader, f)"
      ],
      "metadata": {
        "id": "cyzeg548nHH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data loaders\n",
        "pickle_dir = \"./drive/MyDrive/pickles/\"\n",
        "with open(pickle_dir+'train_dataloader.pkl', 'rb') as f:\n",
        "    train_dataloader = pickle.load(f)\n",
        "\n",
        "with open(pickle_dir+'val_dataloader.pkl', 'rb') as f:\n",
        "    val_dataloader = pickle.load(f)\n",
        "\n",
        "with open(pickle_dir+'test_dataloader.pkl', 'rb') as f:\n",
        "    test_dataloader = pickle.load(f)"
      ],
      "metadata": {
        "id": "Wzy8IzxUnKxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_mobileNet():\n",
        "  # Load pre-trained MobileNetV2 model and replace the classifier layer\n",
        "  model = models.mobilenet_v2(pretrained=True)\n",
        "  #for param in model.parameters():\n",
        "  #    param.requires_grad = False\n",
        "\n",
        "  # Enable gradient computation for all parameters in the classifier layers\n",
        "  #for param in model.classifier.parameters():\n",
        "  #    param.requires_grad = True\n",
        "\n",
        "  # Only train the last three layers\n",
        "  #for param in model.classifier[-3:].parameters():\n",
        "  #    param.requires_grad = True\n",
        "\n",
        "  model.classifier[1] = nn.Linear(model.last_channel, len(labels_dict))\n",
        "  model = model.to(device)\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "TjJUKY51eBJm"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_model_efficientNet():\n",
        "    # Load pre-trained EfficientNet model and replace the classifier layer\n",
        "    model = timm.create_model('efficientnet_b2', pretrained=True)\n",
        "\n",
        "    # Freeze all parameters\n",
        "    #model.requires_grad_(True)\n",
        "\n",
        "    # Unfreeze the last three blocks\n",
        "    # for param in model.blocks[-3:].parameters():\n",
        "    #    param.requires_grad = True\n",
        "\n",
        "    num_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(num_features, len(labels_dict))\n",
        "\n",
        "    model = model.to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "fOjE0-XoaYDK"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_VGG():\n",
        "    model = models.vgg19(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    # Unfreeze the last three layers\n",
        "    for param in model.features[-3:].parameters():\n",
        "        param.requires_grad = True\n",
        "    #print(len(model.parameters() == True))\n",
        "    num_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(num_features, len(labels_dict))  # assuming labels_dict is defined elsewhere\n",
        "\n",
        "    model = model.to(device)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "lWB2UOAsaps_"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_model_densenet():\n",
        "    # Load pre-trained DenseNet model and replace the classifier layer\n",
        "    model = models.densenet201(pretrained=True)\n",
        "\n",
        "    # Freeze all parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Unfreeze the last layer block (you may need to adjust this part to suit your needs)\n",
        "    for param in model.features.denseblock4.parameters():\n",
        "        param.requires_grad = True\n",
        "    \n",
        "    \n",
        "\n",
        "    num_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(num_features, len(labels_dict))  # assuming labels_dict is defined elsewhere\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "6RuS1HvGo-hj"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_model_resnet():\n",
        "    # Load pre-trained ResNet model and replace the classifier layer\n",
        "    model = models.resnet152(pretrained=True)\n",
        "\n",
        "    # Freeze all parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Unfreeze the last layer\n",
        "    for param in model.layer4.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_features, len(labels_dict))  # assuming labels_dict is defined elsewhere\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Vr3IyQ_8f0mB"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_model_alexnet():\n",
        "    # Load pre-trained AlexNet model and replace the classifier layer\n",
        "    model = models.alexnet(pretrained=True)\n",
        "\n",
        "    # Freeze all parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Unfreeze the last layer\n",
        "    for param in model.features[-4:].parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    num_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(num_features, len(labels_dict))  # assuming labels_dict is defined elsewhere\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "-PhfQIDPiJiq"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,epochs):\n",
        "  # Define loss criterion and optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "\n",
        "  model = train_classifier(train_dataloader, val_dataloader, model, criterion, optimizer, device,epochs)\n",
        "\n",
        "  if not os.path.exists('hold_classifier_MobileNet.pth'):\n",
        "      # Save the MobileNetV2 model\n",
        "      model_file = 'hold_classifier_MobileNet.pth'\n",
        "      save_model(model, model_file)\n",
        "\n",
        "      # Load the model for inference\n",
        "      model.load_state_dict(torch.load('hold_classifier_MobileNet.pth'))\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "3RmBQRq7eHib"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if not os.path.exists('hold_classifier_MobileNet.pth'):\n",
        "    # Save the MobileNetV2 model\n",
        "    model_file = 'hold_classifier_MobileNet.pth'\n",
        "    save_model(model, model_file)\n",
        "\n",
        "    # Load the model for inference\n",
        "    model.load_state_dict(torch.load('hold_classifier_MobileNet.pth'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S7Uot81eeKEw"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_mobileNet()\n",
        "trained_model = train_model(model,epochs=3)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "id": "JsWOs-vqeMSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67aea897-36b0-40f8-f6a5-e5afb91d4f2b"
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.068822375659285, Training accuracy: 60.35437430786268%, Validation loss: 2.6527495980262756, Validation accuracy: 52.82051282051282%\n",
            "Epoch: 2, Training loss: 0.6376561378610546, Training accuracy: 78.18383167220377%, Validation loss: 1.2410312741994858, Validation accuracy: 63.58974358974359%\n",
            "Epoch: 3, Training loss: 0.5108591081767246, Training accuracy: 81.28460686600222%, Validation loss: 1.6254142671823502, Validation accuracy: 66.15384615384616%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 66.15384615384616%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.46      0.51        24\n",
            "           1       0.72      0.73      0.72        73\n",
            "           2       1.00      0.85      0.92        27\n",
            "           3       0.38      0.53      0.44        15\n",
            "           4       0.48      0.59      0.53        17\n",
            "           5       0.65      0.62      0.63        39\n",
            "\n",
            "    accuracy                           0.66       195\n",
            "   macro avg       0.63      0.63      0.63       195\n",
            "weighted avg       0.68      0.66      0.67       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[11  7  0  2  1  3]\n",
            " [ 3 53  0  4  6  7]\n",
            " [ 1  2 23  1  0  0]\n",
            " [ 2  3  0  8  1  1]\n",
            " [ 2  3  0  0 10  2]\n",
            " [ 0  6  0  6  3 24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model5 = get_model_mobileNet()\n",
        "trained_model5 = train_model(model5,epochs=5)\n",
        "evaluate_classifier(test_dataloader, trained_model5, device)"
      ],
      "metadata": {
        "id": "5QoDSocN2sox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21cc5900-e645-4b6f-eea5-ea3e07813d92"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.1033947241717372, Training accuracy: 58.91472868217054%, Validation loss: 1.4738274812698364, Validation accuracy: 64.61538461538461%\n",
            "Epoch: 2, Training loss: 0.5990191677521015, Training accuracy: 78.3499446290144%, Validation loss: 1.9151490926742554, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 3, Training loss: 0.4040071614857378, Training accuracy: 86.2126245847176%, Validation loss: 2.091602236032486, Validation accuracy: 62.56410256410256%\n",
            "Epoch: 4, Training loss: 0.33475140148195726, Training accuracy: 88.03986710963456%, Validation loss: 1.3928267359733582, Validation accuracy: 66.15384615384616%\n",
            "Epoch: 5, Training loss: 0.26023830858797864, Training accuracy: 90.91915836101883%, Validation loss: 1.7810733616352081, Validation accuracy: 68.2051282051282%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 68.71794871794872%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.58      0.61        24\n",
            "           1       0.66      0.81      0.72        73\n",
            "           2       0.92      0.81      0.86        27\n",
            "           3       0.58      0.47      0.52        15\n",
            "           4       0.50      0.47      0.48        17\n",
            "           5       0.77      0.62      0.69        39\n",
            "\n",
            "    accuracy                           0.69       195\n",
            "   macro avg       0.68      0.63      0.65       195\n",
            "weighted avg       0.69      0.69      0.68       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[14 10  0  0  0  0]\n",
            " [ 2 59  1  1  8  2]\n",
            " [ 1  1 22  1  0  2]\n",
            " [ 3  4  0  7  0  1]\n",
            " [ 2  5  0  0  8  2]\n",
            " [ 0 11  1  3  0 24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_mobileNet()\n",
        "trained_model = train_model(model,epochs=10)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "id": "BWWXATTd2unX",
        "outputId": "020f6ab4-3cad-4e97-b4fe-949dc379e4b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.0522292174141983, Training accuracy: 60.79734219269103%, Validation loss: 1.7706602811813354, Validation accuracy: 62.56410256410256%\n",
            "Epoch: 2, Training loss: 0.6076736727665211, Training accuracy: 78.01771871539313%, Validation loss: 1.5123671740293503, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 3, Training loss: 0.36622859126534957, Training accuracy: 85.76965669988925%, Validation loss: 1.3979102671146393, Validation accuracy: 65.64102564102564%\n",
            "Epoch: 4, Training loss: 0.3288089853936228, Training accuracy: 87.98449612403101%, Validation loss: 2.29059299826622, Validation accuracy: 66.15384615384616%\n",
            "Epoch: 5, Training loss: 0.2514229725147116, Training accuracy: 90.91915836101883%, Validation loss: 2.408460944890976, Validation accuracy: 64.1025641025641%\n",
            "Epoch: 6, Training loss: 0.1386596741604394, Training accuracy: 95.34883720930233%, Validation loss: 2.176498234272003, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 7, Training loss: 0.0942094883528249, Training accuracy: 96.89922480620154%, Validation loss: 1.7464121580123901, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 8, Training loss: 0.07872224380743914, Training accuracy: 97.17607973421927%, Validation loss: 2.6134660243988037, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 9, Training loss: 0.08234974041838071, Training accuracy: 96.62236987818383%, Validation loss: 2.4099478721618652, Validation accuracy: 65.12820512820512%\n",
            "Epoch: 10, Training loss: 0.15179511667068663, Training accuracy: 94.73975636766335%, Validation loss: 2.771315634250641, Validation accuracy: 66.66666666666667%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 62.05128205128205%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.29      0.38        24\n",
            "           1       0.68      0.62      0.65        73\n",
            "           2       1.00      0.85      0.92        27\n",
            "           3       0.53      0.53      0.53        15\n",
            "           4       0.35      0.65      0.46        17\n",
            "           5       0.57      0.69      0.63        39\n",
            "\n",
            "    accuracy                           0.62       195\n",
            "   macro avg       0.61      0.61      0.59       195\n",
            "weighted avg       0.65      0.62      0.62       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 7  8  0  2  4  3]\n",
            " [ 2 45  0  2 12 12]\n",
            " [ 0  2 23  1  0  1]\n",
            " [ 2  1  0  8  1  3]\n",
            " [ 2  3  0  0 11  1]\n",
            " [ 0  7  0  2  3 27]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_mobileNet()\n",
        "trained_model = train_model(model,epochs=20)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "id": "ssncDcHl2wIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30645ef9-3d81-4da4-f09d-e4ee3b1a0097"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.0350793908382285, Training accuracy: 61.40642303433001%, Validation loss: 1.5941693484783173, Validation accuracy: 56.41025641025641%\n",
            "Epoch: 2, Training loss: 0.5694051333542528, Training accuracy: 79.67884828349945%, Validation loss: 1.457981362938881, Validation accuracy: 61.53846153846154%\n",
            "Epoch: 3, Training loss: 0.4081995548873112, Training accuracy: 85.27131782945736%, Validation loss: 1.5894124507904053, Validation accuracy: 57.43589743589744%\n",
            "Epoch: 4, Training loss: 0.2656496886549325, Training accuracy: 90.58693244739756%, Validation loss: 2.1410719007253647, Validation accuracy: 63.58974358974359%\n",
            "Epoch: 5, Training loss: 0.221304417941077, Training accuracy: 92.19269102990033%, Validation loss: 1.494978904724121, Validation accuracy: 60.0%\n",
            "Epoch: 6, Training loss: 0.22078728547384, Training accuracy: 92.41417497231451%, Validation loss: 2.253840982913971, Validation accuracy: 64.61538461538461%\n",
            "Epoch: 7, Training loss: 0.1555251512548019, Training accuracy: 94.79512735326689%, Validation loss: 1.7600007355213165, Validation accuracy: 65.12820512820512%\n",
            "Epoch: 8, Training loss: 0.1806980267424008, Training accuracy: 93.68770764119601%, Validation loss: 1.2648690342903137, Validation accuracy: 65.12820512820512%\n",
            "Epoch: 9, Training loss: 0.11785026935153994, Training accuracy: 96.62236987818383%, Validation loss: 2.3271337747573853, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 10, Training loss: 0.05520631785600864, Training accuracy: 98.39424141749723%, Validation loss: 2.6087040305137634, Validation accuracy: 64.61538461538461%\n",
            "Epoch: 11, Training loss: 0.11045823705478988, Training accuracy: 97.61904761904762%, Validation loss: 2.602737545967102, Validation accuracy: 64.61538461538461%\n",
            "Epoch: 12, Training loss: 0.31787935567313225, Training accuracy: 89.97785160575859%, Validation loss: 1.8267058432102203, Validation accuracy: 62.05128205128205%\n",
            "Epoch: 13, Training loss: 0.19059716794511367, Training accuracy: 93.24473975636766%, Validation loss: 2.0921913981437683, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 14, Training loss: 0.0857139077551406, Training accuracy: 97.50830564784053%, Validation loss: 2.0468491911888123, Validation accuracy: 69.23076923076923%\n",
            "Epoch: 15, Training loss: 0.06626822467058383, Training accuracy: 97.78516057585826%, Validation loss: 2.585433930158615, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 16, Training loss: 0.07158191617706726, Training accuracy: 97.50830564784053%, Validation loss: 2.0838623046875, Validation accuracy: 68.71794871794872%\n",
            "Epoch: 17, Training loss: 0.053693571304581286, Training accuracy: 98.78183831672204%, Validation loss: 2.3972792625427246, Validation accuracy: 69.23076923076923%\n",
            "Epoch: 18, Training loss: 0.09253086262093536, Training accuracy: 97.06533776301218%, Validation loss: 2.595315247774124, Validation accuracy: 63.58974358974359%\n",
            "Epoch: 19, Training loss: 0.0743922221891839, Training accuracy: 97.23145071982282%, Validation loss: 2.331568419933319, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 20, Training loss: 0.07503878289897895, Training accuracy: 97.50830564784053%, Validation loss: 2.3599585592746735, Validation accuracy: 67.17948717948718%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 61.02564102564103%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.42      0.43        24\n",
            "           1       0.62      0.73      0.67        73\n",
            "           2       0.96      0.81      0.88        27\n",
            "           3       0.50      0.20      0.29        15\n",
            "           4       0.47      0.47      0.47        17\n",
            "           5       0.56      0.59      0.57        39\n",
            "\n",
            "    accuracy                           0.61       195\n",
            "   macro avg       0.59      0.54      0.55       195\n",
            "weighted avg       0.61      0.61      0.60       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10 10  0  0  2  2]\n",
            " [ 6 53  1  1  4  8]\n",
            " [ 0  2 22  0  0  3]\n",
            " [ 5  2  0  3  1  4]\n",
            " [ 2  6  0  0  8  1]\n",
            " [ 0 12  0  2  2 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_efficientNet()\n",
        "trained_model = train_model(model,epochs=3)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k712oeT6b8Ya",
        "outputId": "f6f9cce5-0359-449b-cf81-caa758b5f756"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 0.9346702592126255, Training accuracy: 66.27906976744185%, Validation loss: 1.0351901203393936, Validation accuracy: 68.71794871794872%\n",
            "Epoch: 2, Training loss: 0.3525650866072753, Training accuracy: 89.14728682170542%, Validation loss: 1.2667529731988907, Validation accuracy: 68.2051282051282%\n",
            "Epoch: 3, Training loss: 0.16776757610255275, Training accuracy: 94.18604651162791%, Validation loss: 1.9621455073356628, Validation accuracy: 64.1025641025641%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 65.64102564102564%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.38      0.46        24\n",
            "           1       0.84      0.66      0.74        73\n",
            "           2       0.91      0.78      0.84        27\n",
            "           3       0.36      0.27      0.31        15\n",
            "           4       0.42      0.65      0.51        17\n",
            "           5       0.56      0.90      0.69        39\n",
            "\n",
            "    accuracy                           0.66       195\n",
            "   macro avg       0.62      0.60      0.59       195\n",
            "weighted avg       0.69      0.66      0.66       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 9  4  0  0  8  3]\n",
            " [ 2 48  0  4  6 13]\n",
            " [ 1  0 21  1  0  4]\n",
            " [ 3  1  2  4  1  4]\n",
            " [ 0  2  0  0 11  4]\n",
            " [ 0  2  0  2  0 35]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_efficientNet()\n",
        "trained_model = train_model(model,epochs=5)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij-gzFEWcAAG",
        "outputId": "03a67808-d7d2-48ac-a285-dc39b5ae3a8b"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 0.9273959346886339, Training accuracy: 67.66334440753046%, Validation loss: 1.1647311002016068, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 2, Training loss: 0.31001768697952403, Training accuracy: 90.36544850498339%, Validation loss: 1.5290029048919678, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 3, Training loss: 0.17499440548748807, Training accuracy: 94.46290143964562%, Validation loss: 1.4938266277313232, Validation accuracy: 70.25641025641026%\n",
            "Epoch: 4, Training loss: 0.144726956905476, Training accuracy: 96.29014396456257%, Validation loss: 1.046379268169403, Validation accuracy: 72.82051282051282%\n",
            "Epoch: 5, Training loss: 0.17797577805046377, Training accuracy: 94.73975636766335%, Validation loss: 1.8636125028133392, Validation accuracy: 71.7948717948718%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 68.2051282051282%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.46      0.50        24\n",
            "           1       0.65      0.85      0.73        73\n",
            "           2       1.00      0.85      0.92        27\n",
            "           3       0.78      0.47      0.58        15\n",
            "           4       0.50      0.65      0.56        17\n",
            "           5       0.76      0.49      0.59        39\n",
            "\n",
            "    accuracy                           0.68       195\n",
            "   macro avg       0.71      0.63      0.65       195\n",
            "weighted avg       0.70      0.68      0.68       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[11 11  0  0  2  0]\n",
            " [ 3 62  0  0  6  2]\n",
            " [ 0  3 23  0  0  1]\n",
            " [ 2  3  0  7  1  2]\n",
            " [ 3  2  0  0 11  1]\n",
            " [ 1 15  0  2  2 19]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_efficientNet()\n",
        "trained_model = train_model(model,epochs=10)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS64pJjacBTD",
        "outputId": "a8667ce8-c6d7-44cd-bc7a-f282e40d0d20"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 0.9321486960197317, Training accuracy: 66.00221483942414%, Validation loss: 0.9367904663085938, Validation accuracy: 74.35897435897436%\n",
            "Epoch: 2, Training loss: 0.29147311068814374, Training accuracy: 91.25138427464009%, Validation loss: 2.637096017599106, Validation accuracy: 65.64102564102564%\n",
            "Epoch: 3, Training loss: 0.19412270920543834, Training accuracy: 93.57696566998892%, Validation loss: 1.2983133047819138, Validation accuracy: 70.25641025641026%\n",
            "Epoch: 4, Training loss: 0.13694805330757437, Training accuracy: 95.73643410852713%, Validation loss: 2.2609579265117645, Validation accuracy: 63.07692307692308%\n",
            "Epoch: 5, Training loss: 0.12959158735285545, Training accuracy: 95.9579180509413%, Validation loss: 1.5773551166057587, Validation accuracy: 68.2051282051282%\n",
            "Epoch: 6, Training loss: 0.06298544196861572, Training accuracy: 98.28349944629015%, Validation loss: 2.1603429317474365, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 7, Training loss: 0.11231371875980804, Training accuracy: 97.00996677740864%, Validation loss: 2.3758812844753265, Validation accuracy: 64.1025641025641%\n",
            "Epoch: 8, Training loss: 0.12517418558227605, Training accuracy: 96.23477297895903%, Validation loss: 1.8944490849971771, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 9, Training loss: 0.11536079015711258, Training accuracy: 97.3421926910299%, Validation loss: 2.0331870019435883, Validation accuracy: 66.15384615384616%\n",
            "Epoch: 10, Training loss: 0.12546742636838865, Training accuracy: 96.01328903654485%, Validation loss: 1.6367504000663757, Validation accuracy: 66.15384615384616%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 68.2051282051282%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.42      0.48        24\n",
            "           1       0.69      0.81      0.74        73\n",
            "           2       0.93      0.93      0.93        27\n",
            "           3       0.64      0.47      0.54        15\n",
            "           4       0.45      0.59      0.51        17\n",
            "           5       0.71      0.56      0.63        39\n",
            "\n",
            "    accuracy                           0.68       195\n",
            "   macro avg       0.66      0.63      0.64       195\n",
            "weighted avg       0.68      0.68      0.68       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  8  0  0  4  2]\n",
            " [ 3 59  1  1  7  2]\n",
            " [ 0  0 25  1  0  1]\n",
            " [ 3  1  1  7  0  3]\n",
            " [ 1  5  0  0 10  1]\n",
            " [ 1 13  0  2  1 22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_efficientNet()\n",
        "trained_model = train_model(model,epochs=20)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrvlLTY7urNs",
        "outputId": "894e481a-0119-4f0b-c702-ce816d9c6953"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 0.872700945056718, Training accuracy: 68.6046511627907%, Validation loss: 0.9906402677297592, Validation accuracy: 73.84615384615384%\n",
            "Epoch: 2, Training loss: 0.28199248298488816, Training accuracy: 91.5282392026578%, Validation loss: 1.5252069532871246, Validation accuracy: 67.17948717948718%\n",
            "Epoch: 3, Training loss: 0.2294543370090682, Training accuracy: 92.24806201550388%, Validation loss: 1.9571730196475983, Validation accuracy: 65.12820512820512%\n",
            "Epoch: 4, Training loss: 0.16999169683148121, Training accuracy: 94.73975636766335%, Validation loss: 1.313079059123993, Validation accuracy: 64.61538461538461%\n",
            "Epoch: 5, Training loss: 0.1319006549130226, Training accuracy: 96.29014396456257%, Validation loss: 1.8295849561691284, Validation accuracy: 73.84615384615384%\n",
            "Epoch: 6, Training loss: 0.0981348352185611, Training accuracy: 97.84053156146179%, Validation loss: 1.3715955913066864, Validation accuracy: 76.41025641025641%\n",
            "Epoch: 7, Training loss: 0.09891607695869331, Training accuracy: 97.28682170542636%, Validation loss: 1.7307242453098297, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 8, Training loss: 0.07084794528782368, Training accuracy: 97.72978959025471%, Validation loss: 2.341416746377945, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 9, Training loss: 0.07301864960102429, Training accuracy: 97.95127353266888%, Validation loss: 1.972966343164444, Validation accuracy: 65.64102564102564%\n",
            "Epoch: 10, Training loss: 0.0431552000648888, Training accuracy: 98.94795127353267%, Validation loss: 2.0706659853458405, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 11, Training loss: 0.11231040594906642, Training accuracy: 96.95459579180509%, Validation loss: 2.1683496832847595, Validation accuracy: 72.82051282051282%\n",
            "Epoch: 12, Training loss: 0.13376806804846073, Training accuracy: 96.4562569213732%, Validation loss: 2.4713062942028046, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 13, Training loss: 0.08702668929793711, Training accuracy: 97.23145071982282%, Validation loss: 2.0238737761974335, Validation accuracy: 72.82051282051282%\n",
            "Epoch: 14, Training loss: 0.039291003221196344, Training accuracy: 99.2248062015504%, Validation loss: 2.0005781948566437, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 15, Training loss: 0.08737693258143704, Training accuracy: 97.39756367663344%, Validation loss: 1.421979933977127, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 16, Training loss: 0.06588752803810198, Training accuracy: 98.06201550387597%, Validation loss: 1.9402677118778229, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 17, Training loss: 0.03621920051277969, Training accuracy: 99.28017718715392%, Validation loss: 1.8213598132133484, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 18, Training loss: 0.03806869026498291, Training accuracy: 98.83720930232558%, Validation loss: 2.172564744949341, Validation accuracy: 70.25641025641026%\n",
            "Epoch: 19, Training loss: 0.009841045882584977, Training accuracy: 99.88925802879291%, Validation loss: 2.0481798946857452, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 20, Training loss: 0.014006953135510135, Training accuracy: 99.77851605758582%, Validation loss: 2.058727115392685, Validation accuracy: 75.8974358974359%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 73.33333333333333%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.62      0.64        24\n",
            "           1       0.72      0.86      0.79        73\n",
            "           2       0.96      0.85      0.90        27\n",
            "           3       1.00      0.40      0.57        15\n",
            "           4       0.57      0.47      0.52        17\n",
            "           5       0.68      0.72      0.70        39\n",
            "\n",
            "    accuracy                           0.73       195\n",
            "   macro avg       0.76      0.65      0.69       195\n",
            "weighted avg       0.75      0.73      0.73       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[15  6  0  0  2  1]\n",
            " [ 2 63  0  0  4  4]\n",
            " [ 0  2 23  0  0  2]\n",
            " [ 5  1  0  6  0  3]\n",
            " [ 1  5  0  0  8  3]\n",
            " [ 0 10  1  0  0 28]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_VGG()\n",
        "trained_model = train_model(model,epochs=3)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1xjlGxqcUpg",
        "outputId": "7b9ff5d0-aa67-47dd-9266-0147b4826862"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.3815136654623623, Training accuracy: 47.3421926910299%, Validation loss: 1.09053435921669, Validation accuracy: 66.15384615384616%\n",
            "Epoch: 2, Training loss: 0.9501745680282856, Training accuracy: 65.39313399778516%, Validation loss: 1.0638394206762314, Validation accuracy: 68.2051282051282%\n",
            "Epoch: 3, Training loss: 0.7075542889792343, Training accuracy: 73.20044296788483%, Validation loss: 1.0860777795314789, Validation accuracy: 67.17948717948718%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 65.12820512820512%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.38      0.47        24\n",
            "           1       0.60      0.89      0.71        73\n",
            "           2       0.88      0.78      0.82        27\n",
            "           3       0.50      0.13      0.21        15\n",
            "           4       0.62      0.29      0.40        17\n",
            "           5       0.69      0.64      0.67        39\n",
            "\n",
            "    accuracy                           0.65       195\n",
            "   macro avg       0.66      0.52      0.55       195\n",
            "weighted avg       0.66      0.65      0.62       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 9 13  1  0  0  1]\n",
            " [ 2 65  1  0  3  2]\n",
            " [ 0  3 21  1  0  2]\n",
            " [ 2  6  1  2  0  4]\n",
            " [ 1  9  0  0  5  2]\n",
            " [ 0 13  0  1  0 25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_VGG()\n",
        "trained_model = train_model(model,epochs=5)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayAP8wZrcXrZ",
        "outputId": "e79ad85b-4252-4938-bff3-378a650e4d79"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.3521068692207336, Training accuracy: 46.12403100775194%, Validation loss: 1.235445350408554, Validation accuracy: 65.64102564102564%\n",
            "Epoch: 2, Training loss: 0.9256667392007236, Training accuracy: 65.17165005537099%, Validation loss: 1.1238585412502289, Validation accuracy: 65.64102564102564%\n",
            "Epoch: 3, Training loss: 0.6955751211478792, Training accuracy: 73.75415282392026%, Validation loss: 1.1905182003974915, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 4, Training loss: 0.5781327640188152, Training accuracy: 78.7375415282392%, Validation loss: 1.4986803233623505, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 5, Training loss: 0.40663032798931514, Training accuracy: 84.99446290143965%, Validation loss: 1.3009531795978546, Validation accuracy: 71.28205128205128%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 66.15384615384616%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.62      0.61        24\n",
            "           1       0.71      0.70      0.70        73\n",
            "           2       0.70      0.85      0.77        27\n",
            "           3       0.80      0.27      0.40        15\n",
            "           4       0.48      0.59      0.53        17\n",
            "           5       0.67      0.67      0.67        39\n",
            "\n",
            "    accuracy                           0.66       195\n",
            "   macro avg       0.66      0.62      0.61       195\n",
            "weighted avg       0.67      0.66      0.65       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[15  4  1  0  2  2]\n",
            " [ 4 51  3  0  9  6]\n",
            " [ 0  3 23  0  0  1]\n",
            " [ 3  5  1  4  0  2]\n",
            " [ 3  2  0  0 10  2]\n",
            " [ 0  7  5  1  0 26]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_VGG()\n",
        "trained_model = train_model(model,epochs=10)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG85FD5ucYpu",
        "outputId": "e9f67429-eb7b-4f49-9a4c-961ebceca44a"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.3699009007421032, Training accuracy: 46.56699889258029%, Validation loss: 1.0508307963609695, Validation accuracy: 64.1025641025641%\n",
            "Epoch: 2, Training loss: 1.0562508517298206, Training accuracy: 61.24031007751938%, Validation loss: 1.2050221264362335, Validation accuracy: 66.15384615384616%\n",
            "Epoch: 3, Training loss: 0.7741676281238424, Training accuracy: 70.65337763012181%, Validation loss: 1.016515091061592, Validation accuracy: 71.7948717948718%\n",
            "Epoch: 4, Training loss: 0.6453408424196572, Training accuracy: 76.35658914728683%, Validation loss: 1.0590747743844986, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 5, Training loss: 0.5268838837228972, Training accuracy: 80.0110741971207%, Validation loss: 1.1854523569345474, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 6, Training loss: 0.41363473493477393, Training accuracy: 85.60354374307863%, Validation loss: 1.4340049028396606, Validation accuracy: 70.25641025641026%\n",
            "Epoch: 7, Training loss: 0.31422587920879497, Training accuracy: 87.98449612403101%, Validation loss: 1.1785560846328735, Validation accuracy: 73.84615384615384%\n",
            "Epoch: 8, Training loss: 0.20550671322592373, Training accuracy: 92.63565891472868%, Validation loss: 1.5678761899471283, Validation accuracy: 65.64102564102564%\n",
            "Epoch: 9, Training loss: 0.17707387878206268, Training accuracy: 92.80177187153932%, Validation loss: 1.9976923763751984, Validation accuracy: 71.7948717948718%\n",
            "Epoch: 10, Training loss: 0.11042909060829673, Training accuracy: 96.34551495016612%, Validation loss: 1.9830570966005325, Validation accuracy: 68.71794871794872%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 66.66666666666667%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.50      0.56        24\n",
            "           1       0.68      0.78      0.73        73\n",
            "           2       0.87      0.74      0.80        27\n",
            "           3       0.56      0.33      0.42        15\n",
            "           4       0.41      0.53      0.46        17\n",
            "           5       0.71      0.69      0.70        39\n",
            "\n",
            "    accuracy                           0.67       195\n",
            "   macro avg       0.64      0.60      0.61       195\n",
            "weighted avg       0.67      0.67      0.66       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[12  8  1  0  2  1]\n",
            " [ 3 57  1  0  9  3]\n",
            " [ 0  3 20  2  0  2]\n",
            " [ 2  3  1  5  1  3]\n",
            " [ 2  4  0  0  9  2]\n",
            " [ 0  9  0  2  1 27]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_resnet()\n",
        "trained_model = train_model(model,epochs=3)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn4jVghtf5-r",
        "outputId": "5af808f7-9363-497d-8ccf-2b30378e447f"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 0.9598960465398328, Training accuracy: 64.72868217054264%, Validation loss: 1.4220524728298187, Validation accuracy: 62.56410256410256%\n",
            "Epoch: 2, Training loss: 0.3688853645119174, Training accuracy: 87.87375415282392%, Validation loss: 1.342769831418991, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 3, Training loss: 0.21050604689737845, Training accuracy: 92.80177187153932%, Validation loss: 0.9091637581586838, Validation accuracy: 71.7948717948718%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 70.25641025641026%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.54      0.57        24\n",
            "           1       0.72      0.86      0.78        73\n",
            "           2       0.86      0.89      0.87        27\n",
            "           3       0.44      0.27      0.33        15\n",
            "           4       0.53      0.53      0.53        17\n",
            "           5       0.77      0.62      0.69        39\n",
            "\n",
            "    accuracy                           0.70       195\n",
            "   macro avg       0.65      0.62      0.63       195\n",
            "weighted avg       0.69      0.70      0.69       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[13  8  0  0  3  0]\n",
            " [ 3 63  2  0  3  2]\n",
            " [ 0  0 24  1  0  2]\n",
            " [ 4  3  1  4  1  2]\n",
            " [ 2  5  0  0  9  1]\n",
            " [ 0  9  1  4  1 24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_resnet()\n",
        "trained_model = train_model(model,epochs=5)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6qGFT99gAuZ",
        "outputId": "56555f2f-fac3-4f9c-d100-0ca31ab2f186"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 0.9836982057012361, Training accuracy: 64.45182724252491%, Validation loss: 1.0147064328193665, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 2, Training loss: 0.34074186353847896, Training accuracy: 87.65227021040974%, Validation loss: 1.2127640694379807, Validation accuracy: 67.17948717948718%\n",
            "Epoch: 3, Training loss: 0.17635074990062877, Training accuracy: 94.18604651162791%, Validation loss: 0.870762050151825, Validation accuracy: 74.87179487179488%\n",
            "Epoch: 4, Training loss: 0.14890534058213234, Training accuracy: 95.29346622369879%, Validation loss: 0.9477849826216698, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 5, Training loss: 0.13490611250544415, Training accuracy: 97.00996677740864%, Validation loss: 1.5366940796375275, Validation accuracy: 69.23076923076923%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 67.6923076923077%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.62      0.58        24\n",
            "           1       0.78      0.77      0.77        73\n",
            "           2       0.69      0.93      0.79        27\n",
            "           3       0.50      0.27      0.35        15\n",
            "           4       0.35      0.41      0.38        17\n",
            "           5       0.81      0.64      0.71        39\n",
            "\n",
            "    accuracy                           0.68       195\n",
            "   macro avg       0.61      0.61      0.60       195\n",
            "weighted avg       0.68      0.68      0.67       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[15  3  1  0  5  0]\n",
            " [ 5 56  4  1  5  2]\n",
            " [ 1  0 25  0  0  1]\n",
            " [ 4  1  3  4  1  2]\n",
            " [ 3  5  1  0  7  1]\n",
            " [ 0  7  2  3  2 25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_resnet()\n",
        "trained_model = train_model(model,epochs=10)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpWLkrlvgCUf",
        "outputId": "6593fefd-df54-43cc-f6a1-6334fe692f27"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.0154001610032444, Training accuracy: 62.90143964562569%, Validation loss: 1.1974450796842575, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 2, Training loss: 0.36031386821434414, Training accuracy: 87.59689922480621%, Validation loss: 1.4713086783885956, Validation accuracy: 65.64102564102564%\n",
            "Epoch: 3, Training loss: 0.1650459753542111, Training accuracy: 94.57364341085271%, Validation loss: 1.1751352399587631, Validation accuracy: 70.25641025641026%\n",
            "Epoch: 4, Training loss: 0.15269257642071823, Training accuracy: 95.40420819490586%, Validation loss: 1.6543059349060059, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 5, Training loss: 0.10120271974853401, Training accuracy: 96.78848283499447%, Validation loss: 1.696806013584137, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 6, Training loss: 0.06298565864562988, Training accuracy: 98.56035437430786%, Validation loss: 1.6771423518657684, Validation accuracy: 68.2051282051282%\n",
            "Epoch: 7, Training loss: 0.03797085994276507, Training accuracy: 98.83720930232558%, Validation loss: 1.1081605032086372, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 8, Training loss: 0.08130678831567538, Training accuracy: 98.56035437430786%, Validation loss: 1.3447419106960297, Validation accuracy: 65.64102564102564%\n",
            "Epoch: 9, Training loss: 0.17112110850626025, Training accuracy: 94.13067552602436%, Validation loss: 1.2413905560970306, Validation accuracy: 69.23076923076923%\n",
            "Epoch: 10, Training loss: 0.06841552610798128, Training accuracy: 97.95127353266888%, Validation loss: 1.7811341881752014, Validation accuracy: 74.35897435897436%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 70.25641025641026%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.50      0.56        24\n",
            "           1       0.68      0.95      0.79        73\n",
            "           2       0.76      0.93      0.83        27\n",
            "           3       0.56      0.33      0.42        15\n",
            "           4       0.70      0.41      0.52        17\n",
            "           5       0.86      0.49      0.62        39\n",
            "\n",
            "    accuracy                           0.70       195\n",
            "   macro avg       0.70      0.60      0.62       195\n",
            "weighted avg       0.71      0.70      0.68       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[12  9  1  0  2  0]\n",
            " [ 1 69  2  0  1  0]\n",
            " [ 0  1 25  0  0  1]\n",
            " [ 3  2  4  5  0  1]\n",
            " [ 3  6  0  0  7  1]\n",
            " [ 0 15  1  4  0 19]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_resnet()\n",
        "trained_model = train_model(model,epochs=20)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFyk21AOg2et",
        "outputId": "4b33264c-d847-4ee6-cab8-d711151bc36c"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.1851976352982816, Training accuracy: 57.308970099667775%, Validation loss: 1.3689427421643183, Validation accuracy: 57.94871794871795%\n",
            "Epoch: 2, Training loss: 0.680096819073753, Training accuracy: 75.91362126245848%, Validation loss: 0.8659154222561762, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 3, Training loss: 0.4324862684032558, Training accuracy: 84.9390919158361%, Validation loss: 1.261734196772942, Validation accuracy: 63.58974358974359%\n",
            "Epoch: 4, Training loss: 0.28072028928559967, Training accuracy: 90.2547065337763%, Validation loss: 1.0815186248375819, Validation accuracy: 68.71794871794872%\n",
            "Epoch: 5, Training loss: 0.2183475135372276, Training accuracy: 92.52491694352159%, Validation loss: 1.4864197786037738, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 6, Training loss: 0.20954040523651427, Training accuracy: 93.02325581395348%, Validation loss: 1.176123123902541, Validation accuracy: 67.17948717948718%\n",
            "Epoch: 7, Training loss: 0.1416804132884187, Training accuracy: 95.79180509413068%, Validation loss: 1.2621624641693556, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 8, Training loss: 0.11966432892280605, Training accuracy: 96.17940199335548%, Validation loss: 1.516475993853349, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 9, Training loss: 0.07805567936646232, Training accuracy: 97.78516057585826%, Validation loss: 1.6835023967119365, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 10, Training loss: 0.08753607571701956, Training accuracy: 96.73311184939092%, Validation loss: 1.525933760863084, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 11, Training loss: 0.04454280707039417, Training accuracy: 98.50498338870432%, Validation loss: 1.5630355362708752, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 12, Training loss: 0.05887014847671185, Training accuracy: 98.06201550387597%, Validation loss: 1.4102344535864317, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 13, Training loss: 0.04867090508910988, Training accuracy: 98.33887043189368%, Validation loss: 1.554136294585008, Validation accuracy: 73.33333333333333%\n",
            "Epoch: 14, Training loss: 0.07402907144814004, Training accuracy: 97.39756367663344%, Validation loss: 1.6971908716055064, Validation accuracy: 67.17948717948718%\n",
            "Epoch: 15, Training loss: 0.04972952305580349, Training accuracy: 98.39424141749723%, Validation loss: 2.0302425485390883, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 16, Training loss: 0.06238414635840572, Training accuracy: 97.78516057585826%, Validation loss: 1.6106555920380812, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 17, Training loss: 0.03127787408583274, Training accuracy: 98.7264673311185%, Validation loss: 1.4177277523737688, Validation accuracy: 72.82051282051282%\n",
            "Epoch: 18, Training loss: 0.046964087022831086, Training accuracy: 98.56035437430786%, Validation loss: 2.3947780499091516, Validation accuracy: 63.58974358974359%\n",
            "Epoch: 19, Training loss: 0.09221462423313591, Training accuracy: 97.28682170542636%, Validation loss: 1.9664752001945789, Validation accuracy: 68.2051282051282%\n",
            "Epoch: 20, Training loss: 0.0524076072857674, Training accuracy: 98.33887043189368%, Validation loss: 1.7281886614285982, Validation accuracy: 68.71794871794872%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 66.66666666666667%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.58      0.60        24\n",
            "           1       0.76      0.77      0.76        73\n",
            "           2       0.91      0.78      0.84        27\n",
            "           3       0.44      0.47      0.45        15\n",
            "           4       0.32      0.53      0.40        17\n",
            "           5       0.74      0.59      0.66        39\n",
            "\n",
            "    accuracy                           0.67       195\n",
            "   macro avg       0.63      0.62      0.62       195\n",
            "weighted avg       0.69      0.67      0.68       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[14  3  0  2  4  1]\n",
            " [ 3 56  1  1 10  2]\n",
            " [ 1  1 21  2  1  1]\n",
            " [ 2  2  1  7  0  3]\n",
            " [ 3  4  0  0  9  1]\n",
            " [ 0  8  0  4  4 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_alexnet()\n",
        "trained_model = train_model(model,epochs=3)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdLc3lYSiMYY",
        "outputId": "b2ab3523-7b79-4fea-eecd-386ef67ef0fb"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.2678824416522323, Training accuracy: 50.332225913621265%, Validation loss: 1.2290533781051636, Validation accuracy: 65.12820512820512%\n",
            "Epoch: 2, Training loss: 0.765293346396808, Training accuracy: 69.82281284606866%, Validation loss: 1.1828121989965439, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 3, Training loss: 0.4740357244836873, Training accuracy: 82.11517165005537%, Validation loss: 1.7304570227861404, Validation accuracy: 68.2051282051282%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 64.61538461538461%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.42      0.47        24\n",
            "           1       0.70      0.75      0.72        73\n",
            "           2       0.84      0.78      0.81        27\n",
            "           3       1.00      0.13      0.24        15\n",
            "           4       0.38      0.35      0.36        17\n",
            "           5       0.59      0.82      0.69        39\n",
            "\n",
            "    accuracy                           0.65       195\n",
            "   macro avg       0.67      0.54      0.55       195\n",
            "weighted avg       0.67      0.65      0.63       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  9  0  0  3  2]\n",
            " [ 3 55  2  0  6  7]\n",
            " [ 0  1 21  0  0  5]\n",
            " [ 3  3  2  2  1  4]\n",
            " [ 2  5  0  0  6  4]\n",
            " [ 1  6  0  0  0 32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_alexnet()\n",
        "trained_model = train_model(model,epochs=5)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jalutd8iUw1",
        "outputId": "2d7e2091-799c-48f1-d675-84fd017ecbad"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.2920500656654095, Training accuracy: 48.72646733111849%, Validation loss: 1.5529797673225403, Validation accuracy: 59.48717948717949%\n",
            "Epoch: 2, Training loss: 0.7555756805271938, Training accuracy: 71.70542635658914%, Validation loss: 1.399549663066864, Validation accuracy: 64.1025641025641%\n",
            "Epoch: 3, Training loss: 0.4775946880209035, Training accuracy: 82.61351052048727%, Validation loss: 1.4125100374221802, Validation accuracy: 69.23076923076923%\n",
            "Epoch: 4, Training loss: 0.32399496744418965, Training accuracy: 88.53820598006645%, Validation loss: 2.0065242797136307, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 5, Training loss: 0.2112150063802456, Training accuracy: 92.35880398671097%, Validation loss: 1.647813305258751, Validation accuracy: 64.1025641025641%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 66.66666666666667%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.54      0.50        24\n",
            "           1       0.88      0.63      0.74        73\n",
            "           2       0.83      0.89      0.86        27\n",
            "           3       0.62      0.33      0.43        15\n",
            "           4       0.31      0.59      0.41        17\n",
            "           5       0.70      0.82      0.75        39\n",
            "\n",
            "    accuracy                           0.67       195\n",
            "   macro avg       0.63      0.63      0.61       195\n",
            "weighted avg       0.72      0.67      0.68       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[13  4  1  0  5  1]\n",
            " [ 5 46  3  1 11  7]\n",
            " [ 0  0 24  0  1  2]\n",
            " [ 4  0  1  5  2  3]\n",
            " [ 4  2  0  0 10  1]\n",
            " [ 2  0  0  2  3 32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_alexnet()\n",
        "trained_model = train_model(model,epochs=10)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r5IeWkViPyl",
        "outputId": "5d76c2af-2a9f-4e46-a7ef-f1e6dcd2fb95"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.1908258367003055, Training accuracy: 53.43300110741971%, Validation loss: 1.0598765356200082, Validation accuracy: 64.61538461538461%\n",
            "Epoch: 2, Training loss: 0.7053823952089276, Training accuracy: 72.70210409745293%, Validation loss: 1.1744089722633362, Validation accuracy: 65.12820512820512%\n",
            "Epoch: 3, Training loss: 0.4710814701883416, Training accuracy: 83.38870431893687%, Validation loss: 1.0910744667053223, Validation accuracy: 64.61538461538461%\n",
            "Epoch: 4, Training loss: 0.27338524554905136, Training accuracy: 90.14396456256921%, Validation loss: 1.2737454686846053, Validation accuracy: 65.12820512820512%\n",
            "Epoch: 5, Training loss: 0.19011479308992102, Training accuracy: 93.85382059800665%, Validation loss: 1.2779672145843506, Validation accuracy: 66.15384615384616%\n",
            "Epoch: 6, Training loss: 0.17239771443500854, Training accuracy: 93.96456256921373%, Validation loss: 1.2948667321886336, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 7, Training loss: 0.13009384019594444, Training accuracy: 95.12735326688815%, Validation loss: 1.5903784888131278, Validation accuracy: 65.12820512820512%\n",
            "Epoch: 8, Training loss: 0.08545286741859295, Training accuracy: 96.84385382059801%, Validation loss: 2.0379080091203963, Validation accuracy: 62.05128205128205%\n",
            "Epoch: 9, Training loss: 0.07681608294022449, Training accuracy: 97.95127353266888%, Validation loss: 1.554084096636091, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 10, Training loss: 0.0465186096793204, Training accuracy: 98.56035437430786%, Validation loss: 1.9562828711100988, Validation accuracy: 65.64102564102564%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 69.74358974358974%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.54      0.51        24\n",
            "           1       0.73      0.84      0.78        73\n",
            "           2       0.96      0.89      0.92        27\n",
            "           3       0.50      0.33      0.40        15\n",
            "           4       0.43      0.53      0.47        17\n",
            "           5       0.83      0.62      0.71        39\n",
            "\n",
            "    accuracy                           0.70       195\n",
            "   macro avg       0.66      0.62      0.63       195\n",
            "weighted avg       0.71      0.70      0.70       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[13  8  0  0  3  0]\n",
            " [ 3 61  0  1  7  1]\n",
            " [ 0  2 24  0  0  1]\n",
            " [ 6  0  1  5  1  2]\n",
            " [ 2  5  0  0  9  1]\n",
            " [ 3  7  0  4  1 24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_alexnet()\n",
        "trained_model = train_model(model,epochs=20)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Coopu3Oricoq",
        "outputId": "2b4c6a3b-243d-427b-b5bf-a468f8edec90"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.2179580529530842, Training accuracy: 53.98671096345515%, Validation loss: 1.0619439312389918, Validation accuracy: 65.64102564102564%\n",
            "Epoch: 2, Training loss: 0.6933513932060777, Training accuracy: 74.08637873754152%, Validation loss: 1.0820266689573015, Validation accuracy: 68.2051282051282%\n",
            "Epoch: 3, Training loss: 0.46243443536131007, Training accuracy: 82.89036544850498%, Validation loss: 1.0185822333608354, Validation accuracy: 67.17948717948718%\n",
            "Epoch: 4, Training loss: 0.31267687077062173, Training accuracy: 88.70431893687707%, Validation loss: 1.2470426048551286, Validation accuracy: 63.58974358974359%\n",
            "Epoch: 5, Training loss: 0.19245363902627377, Training accuracy: 93.63233665559247%, Validation loss: 1.388728107724871, Validation accuracy: 62.56410256410256%\n",
            "Epoch: 6, Training loss: 0.15043253039843157, Training accuracy: 95.01661129568106%, Validation loss: 1.6416954398155212, Validation accuracy: 63.07692307692308%\n",
            "Epoch: 7, Training loss: 0.10527598258238613, Training accuracy: 96.34551495016612%, Validation loss: 1.5818962625094823, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 8, Training loss: 0.06866483472843181, Training accuracy: 97.50830564784053%, Validation loss: 1.7737710901669093, Validation accuracy: 66.15384615384616%\n",
            "Epoch: 9, Training loss: 0.08616031190020997, Training accuracy: 97.3421926910299%, Validation loss: 1.6248054504394531, Validation accuracy: 68.71794871794872%\n",
            "Epoch: 10, Training loss: 0.07580096899580799, Training accuracy: 97.67441860465117%, Validation loss: 1.999940173966544, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 11, Training loss: 0.06216478622273395, Training accuracy: 97.95127353266888%, Validation loss: 1.6460022926330566, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 12, Training loss: 0.030413157731425344, Training accuracy: 99.28017718715392%, Validation loss: 1.946883065359933, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 13, Training loss: 0.041895733831711766, Training accuracy: 98.7264673311185%, Validation loss: 1.981078062738691, Validation accuracy: 69.23076923076923%\n",
            "Epoch: 14, Training loss: 0.04385637113693775, Training accuracy: 98.44961240310077%, Validation loss: 2.0717168280056546, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 15, Training loss: 0.043849430941535456, Training accuracy: 98.56035437430786%, Validation loss: 2.3969313757760182, Validation accuracy: 69.23076923076923%\n",
            "Epoch: 16, Training loss: 0.03321418617423134, Training accuracy: 99.05869324473976%, Validation loss: 2.0576707295009067, Validation accuracy: 73.84615384615384%\n",
            "Epoch: 17, Training loss: 0.026307190683151578, Training accuracy: 99.1140642303433%, Validation loss: 1.8284010546548026, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 18, Training loss: 0.05043435061203414, Training accuracy: 98.11738648947951%, Validation loss: 1.9199827483722143, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 19, Training loss: 0.046083083447771504, Training accuracy: 98.56035437430786%, Validation loss: 1.9604898009981429, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 20, Training loss: 0.04851554331508579, Training accuracy: 98.28349944629015%, Validation loss: 2.0820151482309615, Validation accuracy: 65.12820512820512%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 67.17948717948718%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.50      0.47        24\n",
            "           1       0.78      0.73      0.75        73\n",
            "           2       0.83      0.93      0.88        27\n",
            "           3       0.62      0.33      0.43        15\n",
            "           4       0.42      0.47      0.44        17\n",
            "           5       0.65      0.72      0.68        39\n",
            "\n",
            "    accuracy                           0.67       195\n",
            "   macro avg       0.63      0.61      0.61       195\n",
            "weighted avg       0.68      0.67      0.67       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[12  5  1  0  4  2]\n",
            " [ 6 53  1  0  6  7]\n",
            " [ 0  0 25  0  0  2]\n",
            " [ 5  0  2  5  1  2]\n",
            " [ 2  5  0  0  8  2]\n",
            " [ 2  5  1  3  0 28]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_densenet()\n",
        "trained_model = train_model(model,epochs=3)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBO0EZrZpEdh",
        "outputId": "fed1846c-32fb-4670-e004-f4d12077036b"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n",
            "100%|| 77.4M/77.4M [00:00<00:00, 89.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 0.8247645147915544, Training accuracy: 69.82281284606866%, Validation loss: 1.059909924864769, Validation accuracy: 69.23076923076923%\n",
            "Epoch: 2, Training loss: 0.211917189174685, Training accuracy: 93.57696566998892%, Validation loss: 0.976599931716919, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 3, Training loss: 0.06775839273528807, Training accuracy: 98.61572535991141%, Validation loss: 1.1788903623819351, Validation accuracy: 74.35897435897436%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 71.7948717948718%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.46      0.54        24\n",
            "           1       0.71      0.85      0.77        73\n",
            "           2       0.84      0.78      0.81        27\n",
            "           3       0.78      0.47      0.58        15\n",
            "           4       0.44      0.65      0.52        17\n",
            "           5       0.88      0.72      0.79        39\n",
            "\n",
            "    accuracy                           0.72       195\n",
            "   macro avg       0.72      0.65      0.67       195\n",
            "weighted avg       0.74      0.72      0.72       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[11 10  0  0  3  0]\n",
            " [ 1 62  1  0  8  1]\n",
            " [ 0  4 21  0  0  2]\n",
            " [ 3  1  2  7  1  1]\n",
            " [ 2  4  0  0 11  0]\n",
            " [ 0  6  1  2  2 28]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_densenet()\n",
        "trained_model = train_model(model,epochs=5)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhhppXP0pXwV",
        "outputId": "14f1819c-5920-46fb-9552-b88c57d705f9"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 0.8590661420904356, Training accuracy: 66.38981173864894%, Validation loss: 1.090654969215393, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 2, Training loss: 0.23065630706219836, Training accuracy: 93.30011074197121%, Validation loss: 0.9822745621204376, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 3, Training loss: 0.07158285482176419, Training accuracy: 98.33887043189368%, Validation loss: 0.9645752161741257, Validation accuracy: 72.82051282051282%\n",
            "Epoch: 4, Training loss: 0.025394082069396973, Training accuracy: 99.6124031007752%, Validation loss: 1.0340644717216492, Validation accuracy: 70.25641025641026%\n",
            "Epoch: 5, Training loss: 0.02523759370348577, Training accuracy: 99.6124031007752%, Validation loss: 0.8726556375622749, Validation accuracy: 68.71794871794872%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 71.7948717948718%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.50      0.53        24\n",
            "           1       0.72      0.88      0.79        73\n",
            "           2       0.84      0.78      0.81        27\n",
            "           3       0.60      0.60      0.60        15\n",
            "           4       0.67      0.47      0.55        17\n",
            "           5       0.79      0.67      0.72        39\n",
            "\n",
            "    accuracy                           0.72       195\n",
            "   macro avg       0.70      0.65      0.67       195\n",
            "weighted avg       0.72      0.72      0.71       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[12  9  0  1  2  0]\n",
            " [ 4 64  2  0  2  1]\n",
            " [ 0  3 21  1  0  2]\n",
            " [ 2  1  1  9  0  2]\n",
            " [ 3  4  0  0  8  2]\n",
            " [ 0  8  1  4  0 26]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_densenet()\n",
        "trained_model = train_model(model,epochs=10)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CmYCCWLphcS",
        "outputId": "a0a0e08a-2aba-4505-912e-2ce72003bf4f"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 0.8413676619529724, Training accuracy: 68.10631229235881%, Validation loss: 1.2075041234493256, Validation accuracy: 67.17948717948718%\n",
            "Epoch: 2, Training loss: 0.20588201078875312, Training accuracy: 94.57364341085271%, Validation loss: 0.786307580769062, Validation accuracy: 72.82051282051282%\n",
            "Epoch: 3, Training loss: 0.0742627190875596, Training accuracy: 98.2281284606866%, Validation loss: 0.995068147778511, Validation accuracy: 67.17948717948718%\n",
            "Epoch: 4, Training loss: 0.02699102090414742, Training accuracy: 99.44629014396456%, Validation loss: 0.9616588354110718, Validation accuracy: 73.33333333333333%\n",
            "Epoch: 5, Training loss: 0.015328869063021808, Training accuracy: 99.83388704318936%, Validation loss: 0.8615427501499653, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 6, Training loss: 0.014326027458019811, Training accuracy: 99.6124031007752%, Validation loss: 1.2391501367092133, Validation accuracy: 68.71794871794872%\n",
            "Epoch: 7, Training loss: 0.018628263019089555, Training accuracy: 99.66777408637874%, Validation loss: 1.0320273861289024, Validation accuracy: 68.71794871794872%\n",
            "Epoch: 8, Training loss: 0.03846734552644193, Training accuracy: 98.7264673311185%, Validation loss: 1.264068067073822, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 9, Training loss: 0.05966250534201491, Training accuracy: 98.83720930232558%, Validation loss: 1.2636384665966034, Validation accuracy: 74.35897435897436%\n",
            "Epoch: 10, Training loss: 0.09585773382849734, Training accuracy: 97.23145071982282%, Validation loss: 1.7098099291324615, Validation accuracy: 68.71794871794872%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 67.6923076923077%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.50      0.57        24\n",
            "           1       0.67      0.82      0.74        73\n",
            "           2       0.87      0.74      0.80        27\n",
            "           3       0.71      0.33      0.45        15\n",
            "           4       0.42      0.65      0.51        17\n",
            "           5       0.77      0.62      0.69        39\n",
            "\n",
            "    accuracy                           0.68       195\n",
            "   macro avg       0.69      0.61      0.63       195\n",
            "weighted avg       0.70      0.68      0.67       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[12  9  0  0  3  0]\n",
            " [ 0 60  1  0  9  3]\n",
            " [ 0  4 20  0  0  3]\n",
            " [ 4  2  2  5  1  1]\n",
            " [ 2  4  0  0 11  0]\n",
            " [ 0 11  0  2  2 24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_densenet()\n",
        "trained_model = train_model(model,epochs=20)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTcyPQCwvJVw",
        "outputId": "011b9579-7fe6-4113-8b61-d016849b97c3"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 0.8734383624175499, Training accuracy: 66.38981173864894%, Validation loss: 1.2653553783893585, Validation accuracy: 68.2051282051282%\n",
            "Epoch: 2, Training loss: 0.2656413010995963, Training accuracy: 92.24806201550388%, Validation loss: 1.11929751932621, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 3, Training loss: 0.10597261715808819, Training accuracy: 97.28682170542636%, Validation loss: 1.0208176374435425, Validation accuracy: 74.35897435897436%\n",
            "Epoch: 4, Training loss: 0.046826563004789684, Training accuracy: 99.16943521594685%, Validation loss: 1.26620202511549, Validation accuracy: 74.35897435897436%\n",
            "Epoch: 5, Training loss: 0.03552374853913126, Training accuracy: 99.16943521594685%, Validation loss: 1.0763324946165085, Validation accuracy: 71.7948717948718%\n",
            "Epoch: 6, Training loss: 0.01930790259663401, Training accuracy: 99.6124031007752%, Validation loss: 1.3661493062973022, Validation accuracy: 73.84615384615384%\n",
            "Epoch: 7, Training loss: 0.011570278234009084, Training accuracy: 99.94462901439645%, Validation loss: 1.2310316562652588, Validation accuracy: 73.33333333333333%\n",
            "Epoch: 8, Training loss: 0.04938899811582062, Training accuracy: 98.78183831672204%, Validation loss: 1.5310066789388657, Validation accuracy: 74.87179487179488%\n",
            "Epoch: 9, Training loss: 0.05291959547020238, Training accuracy: 97.78516057585826%, Validation loss: 1.4172720462083817, Validation accuracy: 75.38461538461539%\n",
            "Epoch: 10, Training loss: 0.03008100779823445, Training accuracy: 99.05869324473976%, Validation loss: 1.2003099024295807, Validation accuracy: 72.82051282051282%\n",
            "Epoch: 11, Training loss: 0.03060265735658849, Training accuracy: 99.00332225913621%, Validation loss: 1.322805404663086, Validation accuracy: 71.7948717948718%\n",
            "Epoch: 12, Training loss: 0.04347577161187756, Training accuracy: 99.05869324473976%, Validation loss: 2.0102439522743225, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 13, Training loss: 0.03382898704951693, Training accuracy: 99.00332225913621%, Validation loss: 1.334784746170044, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 14, Training loss: 0.041243264628253104, Training accuracy: 99.16943521594685%, Validation loss: 1.4817728251218796, Validation accuracy: 70.25641025641026%\n",
            "Epoch: 15, Training loss: 0.0316806231336347, Training accuracy: 99.16943521594685%, Validation loss: 2.1734382808208466, Validation accuracy: 69.23076923076923%\n",
            "Epoch: 16, Training loss: 0.02094035116746893, Training accuracy: 99.5016611295681%, Validation loss: 1.1503091305494308, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 17, Training loss: 0.014420466746428403, Training accuracy: 99.72314507198229%, Validation loss: 1.1812615543603897, Validation accuracy: 72.82051282051282%\n",
            "Epoch: 18, Training loss: 0.02513764634455458, Training accuracy: 99.5016611295681%, Validation loss: 1.025604709982872, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 19, Training loss: 0.01401349551100605, Training accuracy: 99.77851605758582%, Validation loss: 1.4660192131996155, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 20, Training loss: 0.014502630171622953, Training accuracy: 99.6124031007752%, Validation loss: 1.3210657089948654, Validation accuracy: 72.3076923076923%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 71.28205128205128%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.46      0.50        24\n",
            "           1       0.70      0.86      0.77        73\n",
            "           2       0.92      0.81      0.86        27\n",
            "           3       0.70      0.47      0.56        15\n",
            "           4       0.57      0.71      0.63        17\n",
            "           5       0.80      0.62      0.70        39\n",
            "\n",
            "    accuracy                           0.71       195\n",
            "   macro avg       0.71      0.65      0.67       195\n",
            "weighted avg       0.72      0.71      0.71       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[11  9  0  0  3  1]\n",
            " [ 3 63  1  0  5  1]\n",
            " [ 0  3 22  0  0  2]\n",
            " [ 4  1  1  7  0  2]\n",
            " [ 2  3  0  0 12  0]\n",
            " [ 0 11  0  3  1 24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_dict = {\"crimp\": 0, \"pinch\": 1, \"pocket\": 2, \"jug\": 3, \"edge\": 4, \"sloper\": 5}\n"
      ],
      "metadata": {
        "id": "l9ZrO53d6OAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}