{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "mount_file_id": "1e0ncpuTeKvEQlAARGbsosFbB4BrLwDYp",
      "authorship_tag": "ABX9TyP2/iVBK0ihbjMvS6g+OWCh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0441f75eaa3840e7af048f01d0603037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44aca697238d40669590b1718b4511c1",
              "IPY_MODEL_8691ca2098df4e03b4757c921b8be826",
              "IPY_MODEL_618c19b86c1e4e12a9fa8e6376431075"
            ],
            "layout": "IPY_MODEL_2b1c82fa55854a4187351c46a5f6020c"
          }
        },
        "44aca697238d40669590b1718b4511c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5653e7fccbf44afa8f2a30e548de3b9a",
            "placeholder": "​",
            "style": "IPY_MODEL_bc26d0be49bb43828159c638a9e67074",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "8691ca2098df4e03b4757c921b8be826": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad66e9f5aac3475cba764edfb6e65c8c",
            "max": 36757206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69294120d23d4ae1b04f78c17a4c0bd8",
            "value": 36757206
          }
        },
        "618c19b86c1e4e12a9fa8e6376431075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_632e74764bac4c6e8b4154670c3ead73",
            "placeholder": "​",
            "style": "IPY_MODEL_69dab35bda7f4a85b654d0c6f8edba11",
            "value": " 36.8M/36.8M [00:02&lt;00:00, 16.9MB/s]"
          }
        },
        "2b1c82fa55854a4187351c46a5f6020c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5653e7fccbf44afa8f2a30e548de3b9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc26d0be49bb43828159c638a9e67074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad66e9f5aac3475cba764edfb6e65c8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69294120d23d4ae1b04f78c17a4c0bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "632e74764bac4c6e8b4154670c3ead73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69dab35bda7f4a85b654d0c6f8edba11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orilevi2809/DL/blob/main/project/all_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-h9XCUtKcVha",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7c55fc-7c4a-4a7c-d6b9-7dc31ed04947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 timm-0.9.2\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "!pip install timm\n",
        "from torch.utils.data import Dataset\n",
        "from google.colab import drive\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224 #TODO: check if we can minize the size of the image e.g. 32*32\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "img_dir = \"./drive/MyDrive/labels/type_labels_base/type_labels_base\"\n",
        "labels_dict = {\"crimp\": 0, \"pinch\": 1, \"pocket\": 2, \"sloper\": 3, \"jug\": 4}#,\"jib\":5}#, \"edge\": 5 , : 2\n",
        "inv_labels_dict = {0 : \"crimp\",  1 : \"pinch\",  2 : \"pocket\",  3: \"sloper\",   4 : \"jug\"}#,5:\"jib\"}#, \"edge\": 5 ,\n",
        "class CustomLayer(nn.Module):\n",
        "    def __init__(self, dropout_rate, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.linear = nn.Linear(in_features, out_features) # Change this\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.out_features = out_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x) # Change this\n",
        "        #x = self.softmax(x)\n",
        "        return x\n",
        "class ImagePathDataset(Dataset):\n",
        "    def __init__(self, images, labels, paths):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.paths = paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx], self.paths[idx]\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=1),\n",
        "    transforms.RandomRotation(degrees=(0, 30)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "noise_factor = 0.3  # Define the intensity of the noise\n",
        "\n",
        "train_transform2 = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=.3, hue=.3),\n",
        "    transforms.RandomRotation(degrees=(-40, 40)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda tensor: tensor + torch.randn_like(tensor) * noise_factor),  # Modified lambda function to add noise to a Tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "train_transform4 = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ColorJitter(brightness=.3, hue=.4),\n",
        "    transforms.RandomAutocontrast(),\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
        "    transforms.RandomRotation(degrees=(-50, 50)),\n",
        "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_transform5 = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ColorJitter(brightness=.3, hue=.3),\n",
        "    transforms.RandomRotation(degrees=(-25, 25)),\n",
        "    transforms.ToTensor(),\n",
        "    # Modified lambda function to add noise to a Tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "wKg32gWBdKfR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, model_file):\n",
        "    # Save the model's state dictionary to disk\n",
        "    torch.save(model.state_dict(), model_file)\n"
      ],
      "metadata": {
        "id": "AKU1xHdSdWNc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_paths(img_dir, labels):\n",
        "    data = []\n",
        "    target = []\n",
        "    label_count = {}  # Dictionary to store the count of images for each label\n",
        "    for label in labels:\n",
        "        label_dir = os.path.join(img_dir, str(label))\n",
        "        count = 0  # Initialize the count for the current label\n",
        "        for filename in os.listdir(label_dir):\n",
        "            if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\")or filename.endswith(\".png\"):\n",
        "                img_path = os.path.join(label_dir, filename)\n",
        "                data.append(img_path)\n",
        "                target.append(labels_dict[label])\n",
        "                count += 1  # Increment the count for the current label\n",
        "        label_count[label] = count  # Store the count in the dictionary\n",
        "\n",
        "    # Print the count of images for each label\n",
        "    for label, count in label_count.items():\n",
        "        print(f\"Number of images for label '{label}': {count}\")\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "3m7EQoTqdZQy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img):\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        img = img.numpy()\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "        img = (img * 255).astype(np.uint8)\n",
        "        plt.imshow(img)\n",
        "    else:  # Assuming img is a PIL.Image object\n",
        "        plt.imshow(img)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "d2ASLAkPFOSe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_data(img_paths, labels, transform, augment_unbalanced=False):\n",
        "    if augment_unbalanced:\n",
        "        data = []\n",
        "        target = []\n",
        "        path_list = []\n",
        "        label_counts = {label: 0 for label in set(labels)}\n",
        "        for img_path, label in zip(img_paths, labels):\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            img = test_transform(img)\n",
        "            data.append(img)\n",
        "            path_list.append(img)\n",
        "            target.append(label)\n",
        "            label_counts[label] += 1\n",
        "\n",
        "        is_unbalanced = any(count < max(label_counts.values()) for count in label_counts.values())\n",
        "        print(label_counts)\n",
        "\n",
        "        if is_unbalanced:\n",
        "            print(\"unbalanced\")\n",
        "            augmented_data = []\n",
        "            augmented_target = []\n",
        "            path_list = []\n",
        "            #TODO: add while all equal\n",
        "            max_count = max(label_counts.values())\n",
        "            for img_path, label in zip(img_paths, labels):\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                augmented_data.append(test_transform(img))\n",
        "                augmented_target.append(label)\n",
        "                path_list.append(img_path)\n",
        "                for i in range(1):\n",
        "                    if label_counts[label] < 1*3*max_count:\n",
        "                        augmented_img = train_transform4(img)\n",
        "                        augmented_data.append(augmented_img)\n",
        "                        augmented_target.append(label)\n",
        "                        path_list.append(img_path)\n",
        "                        label_counts[label] += 1\n",
        "                    if label_counts[label] < 1*3*max_count:\n",
        "                        augmented_img = train_transform(img)\n",
        "                        augmented_data.append(augmented_img)\n",
        "                        augmented_target.append(label)\n",
        "                        path_list.append(img_path)\n",
        "                        label_counts[label] += 1\n",
        "                    if label_counts[label] < 1*3*max_count:\n",
        "                        augmented_img = train_transform2(img)\n",
        "                        augmented_data.append(augmented_img)\n",
        "                        augmented_target.append(label)\n",
        "                        path_list.append(img_path)\n",
        "                        label_counts[label] += 1\n",
        "                    if label_counts[label] < 1*3*max_count:\n",
        "                        augmented_img = train_transform5(img)\n",
        "                        augmented_data.append(augmented_img)\n",
        "                        augmented_target.append(label)\n",
        "                        path_list.append(img_path)\n",
        "                        label_counts[label] += 1\n",
        "            print(label_counts)\n",
        "            return augmented_data, augmented_target,path_list\n",
        "        return data, target,path_list\n",
        "    else:\n",
        "        if isinstance(img_paths[0], torch.Tensor):  # Check if data is already in tensor format\n",
        "            return img_paths, labels, img_paths\n",
        "        else:\n",
        "            data = []\n",
        "            target = []\n",
        "            path_list = []\n",
        "            for img_path, label in zip(img_paths, labels):\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                img = test_transform(img)\n",
        "                data.append(img)\n",
        "                target.append(label)\n",
        "                path_list.append(img_path)\n",
        "            return data, target, path_list\n"
      ],
      "metadata": {
        "id": "wJ_D0fcgddJ8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_classifier(train_dataloader, val_dataloader, model, criterion, optimizer, device,scheduler,epochs):\n",
        "    for epoch in range(epochs):  # 10 is the best!!\n",
        "        running_loss = 0.0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        # Switch model to training mode\n",
        "        model.train()\n",
        "\n",
        "        for i, data in enumerate(train_dataloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            # Calculate the number of correct predictions in the current batch\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "        print(f\"Epoch: {epoch + 1}, Learning Rate: {scheduler.get_last_lr()}\")\n",
        "        scheduler.step()\n",
        "        train_accuracy = 100 * total_correct / total_samples\n",
        "\n",
        "        # Switch model to evaluation mode for validation\n",
        "        model.eval()\n",
        "\n",
        "        val_correct = 0\n",
        "        val_samples = 0\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_dataloader, 0):\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                val_samples += labels.size(0)\n",
        "\n",
        "        val_accuracy = 100 * val_correct / val_samples\n",
        "\n",
        "        print(f'Epoch: {epoch + 1}, Training loss: {running_loss / len(train_dataloader)}, Training accuracy: {train_accuracy}%, Validation loss: {val_loss / len(val_dataloader)}, Validation accuracy: {val_accuracy}%')\n",
        "\n",
        "    print('Finished Training')\n",
        "    return model"
      ],
      "metadata": {
        "id": "njyRGLd0dl_I"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classifier(test_dataloader, model, device,img_paths_test):\n",
        "    inverse_normalize = transforms.Normalize(mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],\n",
        "                                              std=[1 / 0.229, 1 / 0.224, 1 / 0.225])\n",
        "\n",
        "    model.eval()  # Switch model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels, paths in test_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            if not torch.all(predicted == labels):\n",
        "                filtered_imgs = images[predicted != labels]\n",
        "                incorrect_indices = (predicted != labels).nonzero().squeeze()\n",
        "                if incorrect_indices.dim() == 0:\n",
        "                    incorrect_indices = incorrect_indices.unsqueeze(\n",
        "                        0)  # Convert the 0D tensor to a 1D tensor with one element\n",
        "                mispredicted_paths = [paths[i] for i in incorrect_indices]\n",
        "                for i in range(len(filtered_imgs)):\n",
        "                    mispredicted_image = filtered_imgs[i]  # Select the first mispredicted image\n",
        "                    mispredicted_path = mispredicted_paths[i]\n",
        "                    label = labels[predicted != labels][i].item()\n",
        "                    prediction = predicted[predicted != labels][i].item()\n",
        "                    path = mispredicted_path.split('/')[-1]\n",
        "                    #plot_image(mispredicted_image,path, label, prediction, inverse_normalize)\n",
        "\n",
        "            # Move tensors to CPU before converting to numpy arrays\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the test images: {accuracy}%')\n",
        "    # Generate classification report and confusion matrix\n",
        "    report = classification_report(all_labels, all_predictions, zero_division=1)\n",
        "    matrix = confusion_matrix(all_labels, all_predictions)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(matrix)\n"
      ],
      "metadata": {
        "id": "9_N6kS2edofh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_paths, labels = load_images_paths(img_dir, labels_dict.keys())\n",
        "# Split the data into train, validation, and test sets (e.g., 70%, 15%, 15%)\n",
        "img_paths_train, img_paths_test, labels_train, labels_test = train_test_split(img_paths, labels, test_size=0.3,\n",
        "                                                                              stratify=labels, random_state=42)\n",
        "img_paths_val, img_paths_test, labels_val, labels_test = train_test_split(img_paths_test, labels_test,\n",
        "                                                                          test_size=0.5, stratify=labels_test,\n",
        "                                                                          random_state=42)\n",
        "data_train, labels_train, img_paths_train = load_image_data(img_paths_train, labels_train, train_transform,\n",
        "                                                            augment_unbalanced=True)\n",
        "data_val, labels_val, img_paths_val = load_image_data(img_paths_val, labels_val, test_transform,\n",
        "                                                      augment_unbalanced=False)\n",
        "data_test1, labels_test1, img_paths_test = load_image_data(img_paths_test, labels_test, test_transform,\n",
        "                                                            augment_unbalanced=False)\n",
        "# Convert to tensors\n",
        "data_train = torch.stack(data_train)\n",
        "labels_train = torch.tensor(labels_train)\n",
        "data_val = torch.stack(data_val)\n",
        "labels_val = torch.tensor(labels_val)\n",
        "data_test = torch.stack(data_test1)\n",
        "labels_test = torch.tensor(labels_test1)\n",
        "# Replace TensorDataset with ImagePathDataset\n",
        "train_dataset = ImagePathDataset(data_train, labels_train, img_paths_train)\n",
        "val_dataset = ImagePathDataset(data_val, labels_val, img_paths_val)\n",
        "test_dataset = ImagePathDataset(data_test, labels_test, img_paths_test)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "-4Qhoj_OdsV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ebc9668-6747-4057-941d-7b1d9f0a3818"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images for label 'crimp': 183\n",
            "Number of images for label 'pinch': 207\n",
            "Number of images for label 'pocket': 234\n",
            "Number of images for label 'sloper': 247\n",
            "Number of images for label 'jug': 209\n",
            "{0: 128, 1: 145, 2: 164, 3: 173, 4: 146}\n",
            "unbalanced\n",
            "{0: 519, 1: 519, 2: 519, 3: 519, 4: 519}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the data loaders\n",
        "pickle_dir = \"./drive/MyDrive/pickles/\"\n",
        "\n",
        "with open(pickle_dir+'train_dataloader.pkl', 'wb') as f:\n",
        "    pickle.dump(train_dataloader, f)\n",
        "\n",
        "with open(pickle_dir+'val_dataloader.pkl', 'wb') as f:\n",
        "    pickle.dump(val_dataloader, f)\n",
        "\n",
        "with open(pickle_dir+'test_dataloader.pkl', 'wb') as f:\n",
        "    pickle.dump(test_dataloader, f)"
      ],
      "metadata": {
        "id": "cyzeg548nHH8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data loaders\n",
        "pickle_dir = \"./drive/MyDrive/pickles/\"\n",
        "with open(pickle_dir+'train_dataloader.pkl', 'rb') as f:\n",
        "    train_dataloader = pickle.load(f)\n",
        "\n",
        "with open(pickle_dir+'val_dataloader.pkl', 'rb') as f:\n",
        "    val_dataloader = pickle.load(f)\n",
        "\n",
        "with open(pickle_dir+'test_dataloader.pkl', 'rb') as f:\n",
        "    test_dataloader = pickle.load(f)"
      ],
      "metadata": {
        "id": "Wzy8IzxUnKxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_mobileNet():\n",
        "    # Load pre-trained MobileNetV2 model and replace the classifier layer\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    dropout_rate = 0.3  # Specify the dropout rate\n",
        "    in_features = model.classifier[-1].in_features  # Replace with the correct number of input features\n",
        "    out_features = len(labels_dict)  # Replace with the correct number of output features\n",
        "    additional_fc_layer = CustomLayer(dropout_rate, in_features, out_features)\n",
        "\n",
        "    model.classifier[-1] = additional_fc_layer\n",
        "\n",
        "    model = model.to(device)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "TjJUKY51eBJm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_model_efficientNet():\n",
        "    # Load pre-trained EfficientNet model and replace the classifier layer\n",
        "    model = timm.create_model('efficientnet_b2', pretrained=True)\n",
        "\n",
        "    # Define the additional FC layer with dropout\n",
        "    dropout_rate = 0.3  # Specify the dropout rate\n",
        "    additional_fc_layer = nn.Sequential(\n",
        "      nn.Dropout(dropout_rate),\n",
        "      nn.Linear(model.classifier.in_features, len(labels_dict)),  # Replace len(labels_dict) with the number of output classes\n",
        "      #nn.Softmax(dim=1)  # Add the softmax activation layer\n",
        "    )\n",
        "\n",
        "    model.classifier = additional_fc_layer\n",
        "\n",
        "    model = model.to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "fOjE0-XoaYDK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_vgg():\n",
        "    # Load pre-trained VGG16 model\n",
        "    model = models.vgg16(pretrained=True)\n",
        "\n",
        "    # Replace the classifier layer\n",
        "    dropout_rate = 0.3  # Specify the dropout rate\n",
        "    in_features = 4096  # Get the number of input features\n",
        "    out_features = len(labels_dict)  # Replace with the correct number of output features\n",
        "    additional_fc_layer = CustomLayer(dropout_rate, in_features, out_features)\n",
        "\n",
        "    model.classifier[6] = additional_fc_layer\n",
        "\n",
        "    model = model.to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "lWB2UOAsaps_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_model_densenet():\n",
        "    # Load pre-trained DenseNet model and replace the classifier layer\n",
        "    model = models.densenet201(pretrained=True)\n",
        "\n",
        "\n",
        "    dropout_rate = 0.3  # Specify the dropout rate\n",
        "    additional_fc_layer = nn.Sequential(\n",
        "      nn.Dropout(dropout_rate),\n",
        "      nn.Linear(model.classifier.in_features, len(labels_dict)),  # Replace len(labels_dict) with the number of output classes\n",
        "      #nn.Softmax(dim=1)  # Add the softmax activation layer\n",
        "    )\n",
        "    model.classifier = additional_fc_layer  # assuming labels_dict is defined elsewhere\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "6RuS1HvGo-hj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_model_resnet():\n",
        "    # Load pre-trained ResNet model and replace the classifier layer\n",
        "    model = models.resnet152(pretrained=True)\n",
        "\n",
        "    dropout_rate = 0.5  # Specify the dropout rate\n",
        "    additional_fc_layer = nn.Sequential(\n",
        "        #nn.Linear(model.fc.in_features, 1000),\n",
        "        nn.Dropout(dropout_rate),\n",
        "        nn.Linear(model.fc.in_features, len(labels_dict)),\n",
        "        #nn.Softmax(dim=1)  # Add the softmax activation layer\n",
        "    )\n",
        "    model.fc = additional_fc_layer\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Vr3IyQ_8f0mB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_model_alexnet():\n",
        "    # Load pre-trained AlexNet model and replace the classifier layer\n",
        "    model = models.alexnet(pretrained=True)\n",
        "\n",
        "    dropout_rate = 0.3  # Specify the dropout rate\n",
        "    additional_fc_layer = nn.Sequential(\n",
        "      nn.Dropout(dropout_rate),\n",
        "      nn.Linear(model.classifier[6].in_features, len(labels_dict)),  # Replace len(labels_dict) with the number of output classes\n",
        "      #nn.Softmax(dim=1)  # Add the softmax activation layer\n",
        "    )\n",
        "    model.classifier[6].in_features = nn.Linear(model.classifier[6].in_features, len(labels_dict))  # assuming labels_dict is defined elsewhere\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "-PhfQIDPiJiq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,epochs,lr):\n",
        "  # Define loss criterion and optimizer\n",
        "  optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "  model = train_classifier(train_dataloader, val_dataloader, model, criterion, optimizer, device, scheduler, epochs)\n",
        "\n",
        "  if not os.path.exists('hold_classifier_MobileNet.pth'):\n",
        "      # Save the MobileNetV2 model\n",
        "      model_file = 'hold_classifier_MobileNet.pth'\n",
        "      save_model(model, model_file)\n",
        "\n",
        "      # Load the model for inference\n",
        "      model.load_state_dict(torch.load('hold_classifier_MobileNet.pth'))\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "3RmBQRq7eHib"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if not os.path.exists('hold_classifier_MobileNet.pth'):\n",
        "    # Save the MobileNetV2 model\n",
        "    model_file = 'hold_classifier_MobileNet.pth'\n",
        "    save_model(model, model_file)\n",
        "\n",
        "    # Load the model for inference\n",
        "    model.load_state_dict(torch.load('hold_classifier_MobileNet.pth'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S7Uot81eeKEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_mobileNet()\n",
        "trained_model = train_model(model,epochs=10,lr = 0.00005)\n",
        "evaluate_classifier(test_dataloader, trained_model, device,img_paths_test)"
      ],
      "metadata": {
        "id": "BWWXATTd2unX",
        "outputId": "fdae751b-45a0-4cef-e852-87e13892b42c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Learning Rate: [5e-05]\n",
            "Epoch: 1, Training loss: 1.1887089071824, Training accuracy: 51.252408477842%, Validation loss: 0.5689842630000341, Validation accuracy: 83.33333333333333%\n",
            "Epoch: 2, Learning Rate: [5e-05]\n",
            "Epoch: 2, Training loss: 0.6666993739513251, Training accuracy: 75.79961464354528%, Validation loss: 0.49889782745213734, Validation accuracy: 81.48148148148148%\n",
            "Epoch: 3, Learning Rate: [5e-05]\n",
            "Epoch: 3, Training loss: 0.4322828610929159, Training accuracy: 84.50867052023122%, Validation loss: 0.4508064856664056, Validation accuracy: 83.95061728395062%\n",
            "Epoch: 4, Learning Rate: [5e-06]\n",
            "Epoch: 4, Training loss: 0.32355558422322456, Training accuracy: 89.28709055876686%, Validation loss: 0.41374443418213297, Validation accuracy: 85.80246913580247%\n",
            "Epoch: 5, Learning Rate: [5e-06]\n",
            "Epoch: 5, Training loss: 0.2687045674140637, Training accuracy: 91.52215799614643%, Validation loss: 0.4022814126773959, Validation accuracy: 86.41975308641975%\n",
            "Epoch: 6, Learning Rate: [5e-06]\n",
            "Epoch: 6, Training loss: 0.24994112892506215, Training accuracy: 91.86897880539499%, Validation loss: 0.38420291395769235, Validation accuracy: 86.41975308641975%\n",
            "Epoch: 7, Learning Rate: [5.000000000000001e-07]\n",
            "Epoch: 7, Training loss: 0.22247253561822267, Training accuracy: 93.41040462427746%, Validation loss: 0.39532936541807084, Validation accuracy: 85.18518518518519%\n",
            "Epoch: 8, Learning Rate: [5.000000000000001e-07]\n",
            "Epoch: 8, Training loss: 0.23160323767134777, Training accuracy: 93.02504816955684%, Validation loss: 0.40217971943673636, Validation accuracy: 85.80246913580247%\n",
            "Epoch: 9, Learning Rate: [5.000000000000001e-07]\n",
            "Epoch: 9, Training loss: 0.23573083559480998, Training accuracy: 92.98651252408477%, Validation loss: 0.384960720404273, Validation accuracy: 87.03703703703704%\n",
            "Epoch: 10, Learning Rate: [5.000000000000001e-08]\n",
            "Epoch: 10, Training loss: 0.21387973099373855, Training accuracy: 93.94990366088632%, Validation loss: 0.4128964912323725, Validation accuracy: 85.80246913580247%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 84.5679012345679%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.82      0.81        28\n",
            "           1       0.92      0.77      0.84        31\n",
            "           2       1.00      0.97      0.99        35\n",
            "           3       0.77      0.92      0.84        37\n",
            "           4       0.76      0.71      0.73        31\n",
            "\n",
            "    accuracy                           0.85       162\n",
            "   macro avg       0.85      0.84      0.84       162\n",
            "weighted avg       0.85      0.85      0.85       162\n",
            "\n",
            "Confusion Matrix:\n",
            "[[23  0  0  0  5]\n",
            " [ 1 24  0  5  1]\n",
            " [ 0  0 34  1  0]\n",
            " [ 0  2  0 34  1]\n",
            " [ 5  0  0  4 22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_mobileNet()\n",
        "trained_model = train_model(model,epochs=20)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "id": "ssncDcHl2wIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30645ef9-3d81-4da4-f09d-e4ee3b1a0097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.0350793908382285, Training accuracy: 61.40642303433001%, Validation loss: 1.5941693484783173, Validation accuracy: 56.41025641025641%\n",
            "Epoch: 2, Training loss: 0.5694051333542528, Training accuracy: 79.67884828349945%, Validation loss: 1.457981362938881, Validation accuracy: 61.53846153846154%\n",
            "Epoch: 3, Training loss: 0.4081995548873112, Training accuracy: 85.27131782945736%, Validation loss: 1.5894124507904053, Validation accuracy: 57.43589743589744%\n",
            "Epoch: 4, Training loss: 0.2656496886549325, Training accuracy: 90.58693244739756%, Validation loss: 2.1410719007253647, Validation accuracy: 63.58974358974359%\n",
            "Epoch: 5, Training loss: 0.221304417941077, Training accuracy: 92.19269102990033%, Validation loss: 1.494978904724121, Validation accuracy: 60.0%\n",
            "Epoch: 6, Training loss: 0.22078728547384, Training accuracy: 92.41417497231451%, Validation loss: 2.253840982913971, Validation accuracy: 64.61538461538461%\n",
            "Epoch: 7, Training loss: 0.1555251512548019, Training accuracy: 94.79512735326689%, Validation loss: 1.7600007355213165, Validation accuracy: 65.12820512820512%\n",
            "Epoch: 8, Training loss: 0.1806980267424008, Training accuracy: 93.68770764119601%, Validation loss: 1.2648690342903137, Validation accuracy: 65.12820512820512%\n",
            "Epoch: 9, Training loss: 0.11785026935153994, Training accuracy: 96.62236987818383%, Validation loss: 2.3271337747573853, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 10, Training loss: 0.05520631785600864, Training accuracy: 98.39424141749723%, Validation loss: 2.6087040305137634, Validation accuracy: 64.61538461538461%\n",
            "Epoch: 11, Training loss: 0.11045823705478988, Training accuracy: 97.61904761904762%, Validation loss: 2.602737545967102, Validation accuracy: 64.61538461538461%\n",
            "Epoch: 12, Training loss: 0.31787935567313225, Training accuracy: 89.97785160575859%, Validation loss: 1.8267058432102203, Validation accuracy: 62.05128205128205%\n",
            "Epoch: 13, Training loss: 0.19059716794511367, Training accuracy: 93.24473975636766%, Validation loss: 2.0921913981437683, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 14, Training loss: 0.0857139077551406, Training accuracy: 97.50830564784053%, Validation loss: 2.0468491911888123, Validation accuracy: 69.23076923076923%\n",
            "Epoch: 15, Training loss: 0.06626822467058383, Training accuracy: 97.78516057585826%, Validation loss: 2.585433930158615, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 16, Training loss: 0.07158191617706726, Training accuracy: 97.50830564784053%, Validation loss: 2.0838623046875, Validation accuracy: 68.71794871794872%\n",
            "Epoch: 17, Training loss: 0.053693571304581286, Training accuracy: 98.78183831672204%, Validation loss: 2.3972792625427246, Validation accuracy: 69.23076923076923%\n",
            "Epoch: 18, Training loss: 0.09253086262093536, Training accuracy: 97.06533776301218%, Validation loss: 2.595315247774124, Validation accuracy: 63.58974358974359%\n",
            "Epoch: 19, Training loss: 0.0743922221891839, Training accuracy: 97.23145071982282%, Validation loss: 2.331568419933319, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 20, Training loss: 0.07503878289897895, Training accuracy: 97.50830564784053%, Validation loss: 2.3599585592746735, Validation accuracy: 67.17948717948718%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 61.02564102564103%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.42      0.43        24\n",
            "           1       0.62      0.73      0.67        73\n",
            "           2       0.96      0.81      0.88        27\n",
            "           3       0.50      0.20      0.29        15\n",
            "           4       0.47      0.47      0.47        17\n",
            "           5       0.56      0.59      0.57        39\n",
            "\n",
            "    accuracy                           0.61       195\n",
            "   macro avg       0.59      0.54      0.55       195\n",
            "weighted avg       0.61      0.61      0.60       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10 10  0  0  2  2]\n",
            " [ 6 53  1  1  4  8]\n",
            " [ 0  2 22  0  0  3]\n",
            " [ 5  2  0  3  1  4]\n",
            " [ 2  6  0  0  8  1]\n",
            " [ 0 12  0  2  2 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_efficientNet()\n",
        "trained_model = train_model(model,epochs=10,lr=0.0001)\n",
        "evaluate_classifier(test_dataloader, trained_model, device,img_paths_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798,
          "referenced_widgets": [
            "0441f75eaa3840e7af048f01d0603037",
            "44aca697238d40669590b1718b4511c1",
            "8691ca2098df4e03b4757c921b8be826",
            "618c19b86c1e4e12a9fa8e6376431075",
            "2b1c82fa55854a4187351c46a5f6020c",
            "5653e7fccbf44afa8f2a30e548de3b9a",
            "bc26d0be49bb43828159c638a9e67074",
            "ad66e9f5aac3475cba764edfb6e65c8c",
            "69294120d23d4ae1b04f78c17a4c0bd8",
            "632e74764bac4c6e8b4154670c3ead73",
            "69dab35bda7f4a85b654d0c6f8edba11"
          ]
        },
        "id": "gS64pJjacBTD",
        "outputId": "532c5d6a-d6c6-47ab-c726-3939066e5641"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/36.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0441f75eaa3840e7af048f01d0603037"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Learning Rate: [0.0001]\n",
            "Epoch: 1, Training loss: 1.0132694054566898, Training accuracy: 61.27167630057804%, Validation loss: 0.6326338797807693, Validation accuracy: 77.77777777777777%\n",
            "Epoch: 2, Learning Rate: [0.0001]\n",
            "Epoch: 2, Training loss: 0.4491519378354916, Training accuracy: 84.58574181117534%, Validation loss: 0.49617333497319904, Validation accuracy: 81.48148148148148%\n",
            "Epoch: 3, Learning Rate: [0.0001]\n",
            "Epoch: 3, Training loss: 0.25933087699401836, Training accuracy: 91.79190751445087%, Validation loss: 0.585948684031055, Validation accuracy: 80.8641975308642%\n",
            "Epoch: 4, Learning Rate: [1e-05]\n",
            "Epoch: 4, Training loss: 0.1567754502967, Training accuracy: 95.45279383429673%, Validation loss: 0.4869310795551255, Validation accuracy: 85.18518518518519%\n",
            "Epoch: 5, Learning Rate: [1e-05]\n",
            "Epoch: 5, Training loss: 0.12128244700340124, Training accuracy: 96.878612716763%, Validation loss: 0.4871495550587064, Validation accuracy: 83.95061728395062%\n",
            "Epoch: 6, Learning Rate: [1e-05]\n",
            "Epoch: 6, Training loss: 0.10716378214410864, Training accuracy: 96.72447013487476%, Validation loss: 0.48306013076078325, Validation accuracy: 83.95061728395062%\n",
            "Epoch: 7, Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch: 7, Training loss: 0.11366366970137907, Training accuracy: 96.76300578034682%, Validation loss: 0.47456720985826994, Validation accuracy: 83.95061728395062%\n",
            "Epoch: 8, Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch: 8, Training loss: 0.1122888463936173, Training accuracy: 96.91714836223507%, Validation loss: 0.45058825115362805, Validation accuracy: 85.18518518518519%\n",
            "Epoch: 9, Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch: 9, Training loss: 0.09880585336341308, Training accuracy: 97.41811175337187%, Validation loss: 0.5045995369908356, Validation accuracy: 83.33333333333333%\n",
            "Epoch: 10, Learning Rate: [1.0000000000000002e-07]\n",
            "Epoch: 10, Training loss: 0.08841608675125126, Training accuracy: 97.91907514450867%, Validation loss: 0.5075479674906958, Validation accuracy: 85.18518518518519%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 91.9753086419753%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.93      0.87        28\n",
            "           1       0.91      0.97      0.94        31\n",
            "           2       0.97      1.00      0.99        35\n",
            "           3       0.92      0.95      0.93        37\n",
            "           4       1.00      0.74      0.85        31\n",
            "\n",
            "    accuracy                           0.92       162\n",
            "   macro avg       0.92      0.92      0.92       162\n",
            "weighted avg       0.93      0.92      0.92       162\n",
            "\n",
            "Confusion Matrix:\n",
            "[[26  2  0  0  0]\n",
            " [ 0 30  0  1  0]\n",
            " [ 0  0 35  0  0]\n",
            " [ 1  0  1 35  0]\n",
            " [ 5  1  0  2 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_efficientNet()\n",
        "trained_model = train_model(model,epochs=20)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrvlLTY7urNs",
        "outputId": "894e481a-0119-4f0b-c702-ce816d9c6953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 0.872700945056718, Training accuracy: 68.6046511627907%, Validation loss: 0.9906402677297592, Validation accuracy: 73.84615384615384%\n",
            "Epoch: 2, Training loss: 0.28199248298488816, Training accuracy: 91.5282392026578%, Validation loss: 1.5252069532871246, Validation accuracy: 67.17948717948718%\n",
            "Epoch: 3, Training loss: 0.2294543370090682, Training accuracy: 92.24806201550388%, Validation loss: 1.9571730196475983, Validation accuracy: 65.12820512820512%\n",
            "Epoch: 4, Training loss: 0.16999169683148121, Training accuracy: 94.73975636766335%, Validation loss: 1.313079059123993, Validation accuracy: 64.61538461538461%\n",
            "Epoch: 5, Training loss: 0.1319006549130226, Training accuracy: 96.29014396456257%, Validation loss: 1.8295849561691284, Validation accuracy: 73.84615384615384%\n",
            "Epoch: 6, Training loss: 0.0981348352185611, Training accuracy: 97.84053156146179%, Validation loss: 1.3715955913066864, Validation accuracy: 76.41025641025641%\n",
            "Epoch: 7, Training loss: 0.09891607695869331, Training accuracy: 97.28682170542636%, Validation loss: 1.7307242453098297, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 8, Training loss: 0.07084794528782368, Training accuracy: 97.72978959025471%, Validation loss: 2.341416746377945, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 9, Training loss: 0.07301864960102429, Training accuracy: 97.95127353266888%, Validation loss: 1.972966343164444, Validation accuracy: 65.64102564102564%\n",
            "Epoch: 10, Training loss: 0.0431552000648888, Training accuracy: 98.94795127353267%, Validation loss: 2.0706659853458405, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 11, Training loss: 0.11231040594906642, Training accuracy: 96.95459579180509%, Validation loss: 2.1683496832847595, Validation accuracy: 72.82051282051282%\n",
            "Epoch: 12, Training loss: 0.13376806804846073, Training accuracy: 96.4562569213732%, Validation loss: 2.4713062942028046, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 13, Training loss: 0.08702668929793711, Training accuracy: 97.23145071982282%, Validation loss: 2.0238737761974335, Validation accuracy: 72.82051282051282%\n",
            "Epoch: 14, Training loss: 0.039291003221196344, Training accuracy: 99.2248062015504%, Validation loss: 2.0005781948566437, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 15, Training loss: 0.08737693258143704, Training accuracy: 97.39756367663344%, Validation loss: 1.421979933977127, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 16, Training loss: 0.06588752803810198, Training accuracy: 98.06201550387597%, Validation loss: 1.9402677118778229, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 17, Training loss: 0.03621920051277969, Training accuracy: 99.28017718715392%, Validation loss: 1.8213598132133484, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 18, Training loss: 0.03806869026498291, Training accuracy: 98.83720930232558%, Validation loss: 2.172564744949341, Validation accuracy: 70.25641025641026%\n",
            "Epoch: 19, Training loss: 0.009841045882584977, Training accuracy: 99.88925802879291%, Validation loss: 2.0481798946857452, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 20, Training loss: 0.014006953135510135, Training accuracy: 99.77851605758582%, Validation loss: 2.058727115392685, Validation accuracy: 75.8974358974359%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 73.33333333333333%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.62      0.64        24\n",
            "           1       0.72      0.86      0.79        73\n",
            "           2       0.96      0.85      0.90        27\n",
            "           3       1.00      0.40      0.57        15\n",
            "           4       0.57      0.47      0.52        17\n",
            "           5       0.68      0.72      0.70        39\n",
            "\n",
            "    accuracy                           0.73       195\n",
            "   macro avg       0.76      0.65      0.69       195\n",
            "weighted avg       0.75      0.73      0.73       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[15  6  0  0  2  1]\n",
            " [ 2 63  0  0  4  4]\n",
            " [ 0  2 23  0  0  2]\n",
            " [ 5  1  0  6  0  3]\n",
            " [ 1  5  0  0  8  3]\n",
            " [ 0 10  1  0  0 28]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_VGG()\n",
        "trained_model = train_model(model,epochs=10)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "DG85FD5ucYpu",
        "outputId": "af5376f4-19ff-412b-909f-9db60e6abc4c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2fc3fd401a32>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_VGG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-8be8d173a9ef>\u001b[0m in \u001b[0;36mget_model_VGG\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     additional_fc_layer = nn.Sequential(\n\u001b[1;32m      6\u001b[0m       \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Replace len(labels_dict) with the number of output classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add the softmax activation layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'in_features'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_resnet()\n",
        "trained_model = train_model(model,epochs=10,lr=0.00005)\n",
        "evaluate_classifier(test_dataloader, trained_model, device,img_paths_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpWLkrlvgCUf",
        "outputId": "24974cd0-aa66-4aca-bd0f-b87581e6842a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Learning Rate: [5e-05]\n",
            "Epoch: 1, Training loss: 0.8745788624194952, Training accuracy: 66.31984585741812%, Validation loss: 0.7643266614703905, Validation accuracy: 74.07407407407408%\n",
            "Epoch: 2, Learning Rate: [5e-05]\n",
            "Epoch: 2, Training loss: 0.42185184322870695, Training accuracy: 85.20231213872832%, Validation loss: 0.483121027238667, Validation accuracy: 79.62962962962963%\n",
            "Epoch: 3, Learning Rate: [5e-05]\n",
            "Epoch: 3, Training loss: 0.25279112823594074, Training accuracy: 92.06165703275529%, Validation loss: 0.4389472111527409, Validation accuracy: 82.09876543209876%\n",
            "Epoch: 4, Learning Rate: [5e-06]\n",
            "Epoch: 4, Training loss: 0.15925277252609912, Training accuracy: 94.99036608863199%, Validation loss: 0.36632786757711855, Validation accuracy: 87.03703703703704%\n",
            "Epoch: 5, Learning Rate: [5e-06]\n",
            "Epoch: 5, Training loss: 0.08685519260712542, Training accuracy: 97.91907514450867%, Validation loss: 0.3471059493188347, Validation accuracy: 87.03703703703704%\n",
            "Epoch: 6, Learning Rate: [5e-06]\n",
            "Epoch: 6, Training loss: 0.06678801647888927, Training accuracy: 98.65125240847784%, Validation loss: 0.3800875518825792, Validation accuracy: 83.95061728395062%\n",
            "Epoch: 7, Learning Rate: [5.000000000000001e-07]\n",
            "Epoch: 7, Training loss: 0.06654770700834119, Training accuracy: 98.65125240847784%, Validation loss: 0.37075880635529757, Validation accuracy: 85.18518518518519%\n",
            "Epoch: 8, Learning Rate: [5.000000000000001e-07]\n",
            "Epoch: 8, Training loss: 0.05211628425293244, Training accuracy: 99.11368015414259%, Validation loss: 0.37201046225215706, Validation accuracy: 83.95061728395062%\n",
            "Epoch: 9, Learning Rate: [5.000000000000001e-07]\n",
            "Epoch: 9, Training loss: 0.058098837685986206, Training accuracy: 99.07514450867052%, Validation loss: 0.4197894837707281, Validation accuracy: 82.09876543209876%\n",
            "Epoch: 10, Learning Rate: [5.000000000000001e-08]\n",
            "Epoch: 10, Training loss: 0.04773205462270058, Training accuracy: 99.38342967244701%, Validation loss: 0.3531018943703246, Validation accuracy: 87.03703703703704%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 87.03703703703704%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.86      0.89        28\n",
            "           1       0.84      0.84      0.84        31\n",
            "           2       0.95      1.00      0.97        35\n",
            "           3       0.87      0.92      0.89        37\n",
            "           4       0.76      0.71      0.73        31\n",
            "\n",
            "    accuracy                           0.87       162\n",
            "   macro avg       0.87      0.86      0.87       162\n",
            "weighted avg       0.87      0.87      0.87       162\n",
            "\n",
            "Confusion Matrix:\n",
            "[[24  0  0  0  4]\n",
            " [ 0 26  1  2  2]\n",
            " [ 0  0 35  0  0]\n",
            " [ 0  2  0 34  1]\n",
            " [ 2  3  1  3 22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_resnet()\n",
        "trained_model = train_model(model,epochs=20)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFyk21AOg2et",
        "outputId": "4b33264c-d847-4ee6-cab8-d711151bc36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.1851976352982816, Training accuracy: 57.308970099667775%, Validation loss: 1.3689427421643183, Validation accuracy: 57.94871794871795%\n",
            "Epoch: 2, Training loss: 0.680096819073753, Training accuracy: 75.91362126245848%, Validation loss: 0.8659154222561762, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 3, Training loss: 0.4324862684032558, Training accuracy: 84.9390919158361%, Validation loss: 1.261734196772942, Validation accuracy: 63.58974358974359%\n",
            "Epoch: 4, Training loss: 0.28072028928559967, Training accuracy: 90.2547065337763%, Validation loss: 1.0815186248375819, Validation accuracy: 68.71794871794872%\n",
            "Epoch: 5, Training loss: 0.2183475135372276, Training accuracy: 92.52491694352159%, Validation loss: 1.4864197786037738, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 6, Training loss: 0.20954040523651427, Training accuracy: 93.02325581395348%, Validation loss: 1.176123123902541, Validation accuracy: 67.17948717948718%\n",
            "Epoch: 7, Training loss: 0.1416804132884187, Training accuracy: 95.79180509413068%, Validation loss: 1.2621624641693556, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 8, Training loss: 0.11966432892280605, Training accuracy: 96.17940199335548%, Validation loss: 1.516475993853349, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 9, Training loss: 0.07805567936646232, Training accuracy: 97.78516057585826%, Validation loss: 1.6835023967119365, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 10, Training loss: 0.08753607571701956, Training accuracy: 96.73311184939092%, Validation loss: 1.525933760863084, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 11, Training loss: 0.04454280707039417, Training accuracy: 98.50498338870432%, Validation loss: 1.5630355362708752, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 12, Training loss: 0.05887014847671185, Training accuracy: 98.06201550387597%, Validation loss: 1.4102344535864317, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 13, Training loss: 0.04867090508910988, Training accuracy: 98.33887043189368%, Validation loss: 1.554136294585008, Validation accuracy: 73.33333333333333%\n",
            "Epoch: 14, Training loss: 0.07402907144814004, Training accuracy: 97.39756367663344%, Validation loss: 1.6971908716055064, Validation accuracy: 67.17948717948718%\n",
            "Epoch: 15, Training loss: 0.04972952305580349, Training accuracy: 98.39424141749723%, Validation loss: 2.0302425485390883, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 16, Training loss: 0.06238414635840572, Training accuracy: 97.78516057585826%, Validation loss: 1.6106555920380812, Validation accuracy: 67.6923076923077%\n",
            "Epoch: 17, Training loss: 0.03127787408583274, Training accuracy: 98.7264673311185%, Validation loss: 1.4177277523737688, Validation accuracy: 72.82051282051282%\n",
            "Epoch: 18, Training loss: 0.046964087022831086, Training accuracy: 98.56035437430786%, Validation loss: 2.3947780499091516, Validation accuracy: 63.58974358974359%\n",
            "Epoch: 19, Training loss: 0.09221462423313591, Training accuracy: 97.28682170542636%, Validation loss: 1.9664752001945789, Validation accuracy: 68.2051282051282%\n",
            "Epoch: 20, Training loss: 0.0524076072857674, Training accuracy: 98.33887043189368%, Validation loss: 1.7281886614285982, Validation accuracy: 68.71794871794872%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 66.66666666666667%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.58      0.60        24\n",
            "           1       0.76      0.77      0.76        73\n",
            "           2       0.91      0.78      0.84        27\n",
            "           3       0.44      0.47      0.45        15\n",
            "           4       0.32      0.53      0.40        17\n",
            "           5       0.74      0.59      0.66        39\n",
            "\n",
            "    accuracy                           0.67       195\n",
            "   macro avg       0.63      0.62      0.62       195\n",
            "weighted avg       0.69      0.67      0.68       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[14  3  0  2  4  1]\n",
            " [ 3 56  1  1 10  2]\n",
            " [ 1  1 21  2  1  1]\n",
            " [ 2  2  1  7  0  3]\n",
            " [ 3  4  0  0  9  1]\n",
            " [ 0  8  0  4  4 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_alexnet()\n",
        "trained_model = train_model(model,epochs=10,lr=0.00005)\n",
        "evaluate_classifier(test_dataloader, trained_model, device,img_paths_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r5IeWkViPyl",
        "outputId": "403a650a-a747-4659-87f8-8edadce3190d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Learning Rate: [5e-05]\n",
            "Epoch: 1, Training loss: 1.6084787657627693, Training accuracy: 46.859344894026975%, Validation loss: 0.8037562725089845, Validation accuracy: 70.37037037037037%\n",
            "Epoch: 2, Learning Rate: [5e-05]\n",
            "Epoch: 2, Training loss: 0.6990604210816896, Training accuracy: 72.52408477842003%, Validation loss: 0.7570531417926153, Validation accuracy: 79.01234567901234%\n",
            "Epoch: 3, Learning Rate: [5e-05]\n",
            "Epoch: 3, Training loss: 0.4505840495458016, Training accuracy: 82.92870905587668%, Validation loss: 0.9089510185377938, Validation accuracy: 72.8395061728395%\n",
            "Epoch: 4, Learning Rate: [5e-06]\n",
            "Epoch: 4, Training loss: 0.2356823197207772, Training accuracy: 92.06165703275529%, Validation loss: 0.7892801215251287, Validation accuracy: 76.54320987654322%\n",
            "Epoch: 5, Learning Rate: [5e-06]\n",
            "Epoch: 5, Training loss: 0.17901525038867616, Training accuracy: 94.29672447013488%, Validation loss: 0.8141397244873501, Validation accuracy: 75.30864197530865%\n",
            "Epoch: 6, Learning Rate: [5e-06]\n",
            "Epoch: 6, Training loss: 0.1601945969910146, Training accuracy: 94.41233140655106%, Validation loss: 0.8450247659569695, Validation accuracy: 75.30864197530865%\n",
            "Epoch: 7, Learning Rate: [5.000000000000001e-07]\n",
            "Epoch: 7, Training loss: 0.1348713618230361, Training accuracy: 95.91522157996147%, Validation loss: 0.8376034711088453, Validation accuracy: 75.92592592592592%\n",
            "Epoch: 8, Learning Rate: [5.000000000000001e-07]\n",
            "Epoch: 8, Training loss: 0.1313861139322846, Training accuracy: 96.0693641618497%, Validation loss: 0.8412250379721323, Validation accuracy: 75.30864197530865%\n",
            "Epoch: 9, Learning Rate: [5.000000000000001e-07]\n",
            "Epoch: 9, Training loss: 0.1245934726509194, Training accuracy: 96.37764932562621%, Validation loss: 0.8446575917658352, Validation accuracy: 75.92592592592592%\n",
            "Epoch: 10, Learning Rate: [5.000000000000001e-08]\n",
            "Epoch: 10, Training loss: 0.12301021847551545, Training accuracy: 96.0693641618497%, Validation loss: 0.8449838842664447, Validation accuracy: 75.92592592592592%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 81.48148148148148%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.64      0.72        28\n",
            "           1       0.86      0.77      0.81        31\n",
            "           2       0.92      0.94      0.93        35\n",
            "           3       0.79      0.92      0.85        37\n",
            "           4       0.70      0.74      0.72        31\n",
            "\n",
            "    accuracy                           0.81       162\n",
            "   macro avg       0.82      0.80      0.81       162\n",
            "weighted avg       0.82      0.81      0.81       162\n",
            "\n",
            "Confusion Matrix:\n",
            "[[18  1  1  0  8]\n",
            " [ 2 24  0  3  2]\n",
            " [ 0  0 33  2  0]\n",
            " [ 0  1  2 34  0]\n",
            " [ 2  2  0  4 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_alexnet()\n",
        "trained_model = train_model(model,epochs=20)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Coopu3Oricoq",
        "outputId": "2b4c6a3b-243d-427b-b5bf-a468f8edec90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 1.2179580529530842, Training accuracy: 53.98671096345515%, Validation loss: 1.0619439312389918, Validation accuracy: 65.64102564102564%\n",
            "Epoch: 2, Training loss: 0.6933513932060777, Training accuracy: 74.08637873754152%, Validation loss: 1.0820266689573015, Validation accuracy: 68.2051282051282%\n",
            "Epoch: 3, Training loss: 0.46243443536131007, Training accuracy: 82.89036544850498%, Validation loss: 1.0185822333608354, Validation accuracy: 67.17948717948718%\n",
            "Epoch: 4, Training loss: 0.31267687077062173, Training accuracy: 88.70431893687707%, Validation loss: 1.2470426048551286, Validation accuracy: 63.58974358974359%\n",
            "Epoch: 5, Training loss: 0.19245363902627377, Training accuracy: 93.63233665559247%, Validation loss: 1.388728107724871, Validation accuracy: 62.56410256410256%\n",
            "Epoch: 6, Training loss: 0.15043253039843157, Training accuracy: 95.01661129568106%, Validation loss: 1.6416954398155212, Validation accuracy: 63.07692307692308%\n",
            "Epoch: 7, Training loss: 0.10527598258238613, Training accuracy: 96.34551495016612%, Validation loss: 1.5818962625094823, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 8, Training loss: 0.06866483472843181, Training accuracy: 97.50830564784053%, Validation loss: 1.7737710901669093, Validation accuracy: 66.15384615384616%\n",
            "Epoch: 9, Training loss: 0.08616031190020997, Training accuracy: 97.3421926910299%, Validation loss: 1.6248054504394531, Validation accuracy: 68.71794871794872%\n",
            "Epoch: 10, Training loss: 0.07580096899580799, Training accuracy: 97.67441860465117%, Validation loss: 1.999940173966544, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 11, Training loss: 0.06216478622273395, Training accuracy: 97.95127353266888%, Validation loss: 1.6460022926330566, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 12, Training loss: 0.030413157731425344, Training accuracy: 99.28017718715392%, Validation loss: 1.946883065359933, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 13, Training loss: 0.041895733831711766, Training accuracy: 98.7264673311185%, Validation loss: 1.981078062738691, Validation accuracy: 69.23076923076923%\n",
            "Epoch: 14, Training loss: 0.04385637113693775, Training accuracy: 98.44961240310077%, Validation loss: 2.0717168280056546, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 15, Training loss: 0.043849430941535456, Training accuracy: 98.56035437430786%, Validation loss: 2.3969313757760182, Validation accuracy: 69.23076923076923%\n",
            "Epoch: 16, Training loss: 0.03321418617423134, Training accuracy: 99.05869324473976%, Validation loss: 2.0576707295009067, Validation accuracy: 73.84615384615384%\n",
            "Epoch: 17, Training loss: 0.026307190683151578, Training accuracy: 99.1140642303433%, Validation loss: 1.8284010546548026, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 18, Training loss: 0.05043435061203414, Training accuracy: 98.11738648947951%, Validation loss: 1.9199827483722143, Validation accuracy: 70.76923076923077%\n",
            "Epoch: 19, Training loss: 0.046083083447771504, Training accuracy: 98.56035437430786%, Validation loss: 1.9604898009981429, Validation accuracy: 66.66666666666667%\n",
            "Epoch: 20, Training loss: 0.04851554331508579, Training accuracy: 98.28349944629015%, Validation loss: 2.0820151482309615, Validation accuracy: 65.12820512820512%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 67.17948717948718%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.50      0.47        24\n",
            "           1       0.78      0.73      0.75        73\n",
            "           2       0.83      0.93      0.88        27\n",
            "           3       0.62      0.33      0.43        15\n",
            "           4       0.42      0.47      0.44        17\n",
            "           5       0.65      0.72      0.68        39\n",
            "\n",
            "    accuracy                           0.67       195\n",
            "   macro avg       0.63      0.61      0.61       195\n",
            "weighted avg       0.68      0.67      0.67       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[12  5  1  0  4  2]\n",
            " [ 6 53  1  0  6  7]\n",
            " [ 0  0 25  0  0  2]\n",
            " [ 5  0  2  5  1  2]\n",
            " [ 2  5  0  0  8  2]\n",
            " [ 2  5  1  3  0 28]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_densenet()\n",
        "trained_model = train_model(model,epochs=10,lr = 0.00003)\n",
        "evaluate_classifier(test_dataloader, trained_model, device,img_paths_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CmYCCWLphcS",
        "outputId": "54dfc0be-eb55-40b6-da39-5acd80fe7afb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Learning Rate: [1e-05]\n",
            "Epoch: 1, Training loss: 1.4515467194410472, Training accuracy: 40.23121387283237%, Validation loss: 1.071089086078462, Validation accuracy: 64.19753086419753%\n",
            "Epoch: 2, Learning Rate: [1e-05]\n",
            "Epoch: 2, Training loss: 0.9923415849759029, Training accuracy: 66.6281310211946%, Validation loss: 0.7179636827536992, Validation accuracy: 78.39506172839506%\n",
            "Epoch: 3, Learning Rate: [1e-05]\n",
            "Epoch: 3, Training loss: 0.6994571438202491, Training accuracy: 77.57225433526011%, Validation loss: 0.5449585701738086, Validation accuracy: 81.48148148148148%\n",
            "Epoch: 4, Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch: 4, Training loss: 0.5517344456911087, Training accuracy: 84.08477842003853%, Validation loss: 0.5249591604584739, Validation accuracy: 82.71604938271605%\n",
            "Epoch: 5, Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch: 5, Training loss: 0.5246119435475423, Training accuracy: 84.47013487475915%, Validation loss: 0.5176957157396135, Validation accuracy: 83.33333333333333%\n",
            "Epoch: 6, Learning Rate: [1.0000000000000002e-06]\n",
            "Epoch: 6, Training loss: 0.5121712564505063, Training accuracy: 85.626204238921%, Validation loss: 0.49751366887773785, Validation accuracy: 83.95061728395062%\n",
            "Epoch: 7, Learning Rate: [1.0000000000000002e-07]\n",
            "Epoch: 7, Training loss: 0.4932519963612923, Training accuracy: 86.78227360308286%, Validation loss: 0.4944407514163426, Validation accuracy: 82.71604938271605%\n",
            "Epoch: 8, Learning Rate: [1.0000000000000002e-07]\n",
            "Epoch: 8, Training loss: 0.4975595902479612, Training accuracy: 85.51059730250482%, Validation loss: 0.48889363805452984, Validation accuracy: 83.95061728395062%\n",
            "Epoch: 9, Learning Rate: [1.0000000000000002e-07]\n",
            "Epoch: 9, Training loss: 0.4930948774172709, Training accuracy: 86.16570327552986%, Validation loss: 0.5023131853058225, Validation accuracy: 83.33333333333333%\n",
            "Epoch: 10, Learning Rate: [1.0000000000000004e-08]\n",
            "Epoch: 10, Training loss: 0.47811697292786376, Training accuracy: 86.66666666666667%, Validation loss: 0.487103052792095, Validation accuracy: 83.95061728395062%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 87.65432098765432%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.68      0.76        28\n",
            "           1       0.84      1.00      0.91        31\n",
            "           2       1.00      1.00      1.00        35\n",
            "           3       0.92      0.89      0.90        37\n",
            "           4       0.75      0.77      0.76        31\n",
            "\n",
            "    accuracy                           0.88       162\n",
            "   macro avg       0.87      0.87      0.87       162\n",
            "weighted avg       0.88      0.88      0.87       162\n",
            "\n",
            "Confusion Matrix:\n",
            "[[19  1  0  0  8]\n",
            " [ 0 31  0  0  0]\n",
            " [ 0  0 35  0  0]\n",
            " [ 1  3  0 33  0]\n",
            " [ 2  2  0  3 24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model_densenet()\n",
        "trained_model = train_model(model,epochs=20)\n",
        "evaluate_classifier(test_dataloader, trained_model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTcyPQCwvJVw",
        "outputId": "011b9579-7fe6-4113-8b61-d016849b97c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training loss: 0.8734383624175499, Training accuracy: 66.38981173864894%, Validation loss: 1.2653553783893585, Validation accuracy: 68.2051282051282%\n",
            "Epoch: 2, Training loss: 0.2656413010995963, Training accuracy: 92.24806201550388%, Validation loss: 1.11929751932621, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 3, Training loss: 0.10597261715808819, Training accuracy: 97.28682170542636%, Validation loss: 1.0208176374435425, Validation accuracy: 74.35897435897436%\n",
            "Epoch: 4, Training loss: 0.046826563004789684, Training accuracy: 99.16943521594685%, Validation loss: 1.26620202511549, Validation accuracy: 74.35897435897436%\n",
            "Epoch: 5, Training loss: 0.03552374853913126, Training accuracy: 99.16943521594685%, Validation loss: 1.0763324946165085, Validation accuracy: 71.7948717948718%\n",
            "Epoch: 6, Training loss: 0.01930790259663401, Training accuracy: 99.6124031007752%, Validation loss: 1.3661493062973022, Validation accuracy: 73.84615384615384%\n",
            "Epoch: 7, Training loss: 0.011570278234009084, Training accuracy: 99.94462901439645%, Validation loss: 1.2310316562652588, Validation accuracy: 73.33333333333333%\n",
            "Epoch: 8, Training loss: 0.04938899811582062, Training accuracy: 98.78183831672204%, Validation loss: 1.5310066789388657, Validation accuracy: 74.87179487179488%\n",
            "Epoch: 9, Training loss: 0.05291959547020238, Training accuracy: 97.78516057585826%, Validation loss: 1.4172720462083817, Validation accuracy: 75.38461538461539%\n",
            "Epoch: 10, Training loss: 0.03008100779823445, Training accuracy: 99.05869324473976%, Validation loss: 1.2003099024295807, Validation accuracy: 72.82051282051282%\n",
            "Epoch: 11, Training loss: 0.03060265735658849, Training accuracy: 99.00332225913621%, Validation loss: 1.322805404663086, Validation accuracy: 71.7948717948718%\n",
            "Epoch: 12, Training loss: 0.04347577161187756, Training accuracy: 99.05869324473976%, Validation loss: 2.0102439522743225, Validation accuracy: 69.74358974358974%\n",
            "Epoch: 13, Training loss: 0.03382898704951693, Training accuracy: 99.00332225913621%, Validation loss: 1.334784746170044, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 14, Training loss: 0.041243264628253104, Training accuracy: 99.16943521594685%, Validation loss: 1.4817728251218796, Validation accuracy: 70.25641025641026%\n",
            "Epoch: 15, Training loss: 0.0316806231336347, Training accuracy: 99.16943521594685%, Validation loss: 2.1734382808208466, Validation accuracy: 69.23076923076923%\n",
            "Epoch: 16, Training loss: 0.02094035116746893, Training accuracy: 99.5016611295681%, Validation loss: 1.1503091305494308, Validation accuracy: 71.28205128205128%\n",
            "Epoch: 17, Training loss: 0.014420466746428403, Training accuracy: 99.72314507198229%, Validation loss: 1.1812615543603897, Validation accuracy: 72.82051282051282%\n",
            "Epoch: 18, Training loss: 0.02513764634455458, Training accuracy: 99.5016611295681%, Validation loss: 1.025604709982872, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 19, Training loss: 0.01401349551100605, Training accuracy: 99.77851605758582%, Validation loss: 1.4660192131996155, Validation accuracy: 72.3076923076923%\n",
            "Epoch: 20, Training loss: 0.014502630171622953, Training accuracy: 99.6124031007752%, Validation loss: 1.3210657089948654, Validation accuracy: 72.3076923076923%\n",
            "Finished Training\n",
            "Accuracy of the model on the test images: 71.28205128205128%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.46      0.50        24\n",
            "           1       0.70      0.86      0.77        73\n",
            "           2       0.92      0.81      0.86        27\n",
            "           3       0.70      0.47      0.56        15\n",
            "           4       0.57      0.71      0.63        17\n",
            "           5       0.80      0.62      0.70        39\n",
            "\n",
            "    accuracy                           0.71       195\n",
            "   macro avg       0.71      0.65      0.67       195\n",
            "weighted avg       0.72      0.71      0.71       195\n",
            "\n",
            "Confusion Matrix:\n",
            "[[11  9  0  0  3  1]\n",
            " [ 3 63  1  0  5  1]\n",
            " [ 0  3 22  0  0  2]\n",
            " [ 4  1  1  7  0  2]\n",
            " [ 2  3  0  0 12  0]\n",
            " [ 0 11  0  3  1 24]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_dict = {\"crimp\": 0, \"pinch\": 1, \"pocket\": 2, \"jug\": 3, \"edge\": 4, \"sloper\": 5}\n"
      ],
      "metadata": {
        "id": "l9ZrO53d6OAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}